{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Installing & Importing Necessary Libraries (To be run only after running FineTuning Notebook)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk rouge_score gensim pytorch_pretrained_bert bert-score moverscore scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from moverscore_v2 import word_mover_score\n",
    "from bert_score import score as bert_score\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Loading Original & PEFT Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f201a029564419d977c0f7824716f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading original model\n",
    "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0980b4a31e4a3cbebc7d622e030ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading PEFT model\n",
    "PEFT_MODEL = \"/teamspace/studios/this_studio/falcon-7b-sharded-bf16-finetuned-treccast\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n",
    "\n",
    "peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "peft_tokenizer.pad_token = peft_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> PEFT Inference Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "<HUMAN>: What is COP26?\n",
      "<ASSISTANT>: COP26 is the 26th Conference of the Parties to the United Nations Framework Convention on Climate Change.\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "<HUMAN>: What is COP26?\n",
      "<ASSISTANT>: The name of the COP event series is \"COP26\". The twenty-sixth conference of the parties (to the United Nations Convention on Climate Change) will be held at the Scottish Event Campus in Glasgow, Scotland, from October 31 to November 12, 2021. It is a governmental summit about climate change and its mitigation and adaptation measures. Approximately 195 countries are expected to participate as delegates. The term \"COP\" is often misused as a synonym for \"climate change conference\", although the Conference of the Parties is used to refer all the way back to the 1992 UNFCCC Kyoto Protocol! A conference is much bigger than a meeting. The proper noun \"conference of the parties\" indicates an important gathering involving many events or activities. The phrase \"climate change conference\" is more commonly seen than \"COP\", even though the former is inaccurate\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "<HUMAN>: How would climate change affect countries?\n",
      "<ASSISTANT>: It would affect countries in many ways.\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "<HUMAN>: How would climate change affect countries?\n",
      "<ASSISTANT>: Climate change will have a big impact now on our planet and all life, both on land and in oceans. It is likely that greenhouse emissions will continue to influence the temperature on Earth for many centuries to come. Global temperatures are expected to rise for decades or centuries to come. The effects of climate change will be felt everywhere on Earth and by all species. Increasing temperatures will very probably have serious negative impacts on human health and on the quality of our environment. \n",
      " Climate change could seriously harm the ability of humanity to feed itself. Changing weather patterns, together with other factors such as the disappearance of the Amazon rainforest, will all play their part. Already rising seas could mean that more of the world’s food growing areas are lost due to expanding coastal deserts caused by the loss of the West African monsoon each year. By the end of the twenty-first century, the average global temperature is predicted to be between 2.\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "<HUMAN>: How do you mnake your own Deodorant?\n",
      "<ASSISTANT>: I use a mixture of baking soda, cornstarch, and water.\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "<HUMAN>: How do you mnake your own Deodorant?\n",
      "<ASSISTANT>: There are endless recipes for deodorants on the web. Here’s one example. You’ll need: 3 Tablespoons baking soda; 2 tablespoons cocoa butter; 1 tablespoon shea butter; and 4 drops of essential oil of your choice (eggs, lavender, or chamomile are all traditional choices). Using a double bowl to make this will make it easier to mix. First, melt the butters by placing them in a heatproof container and setting it over a saucepan of hot water so that they can be stirred every thirty seconds until completely melted. Do not let the mixture touch the water, as it could become lumpy. Once fully melted, remove from the stove and add to it three tablespoons of baking soda. Use a silicone spatula or wooden spoon to stir thoroughly. In a separate bowl, measure out your coconut oil if necessary. To prevent it from melting,\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(query):\n",
    "    system_prompt = \"\"\"Answer the following question truthfully.\n",
    "If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
    "If the question is too complex, respond 'Kindly, consult the author.'.\"\"\"\n",
    "\n",
    "    user_prompt = f\"<HUMAN>: {query}\\n<ASSISTANT>:\"\n",
    "\n",
    "    final_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    device = \"cuda:0\"\n",
    "    dashline = \"-\" * 50\n",
    "\n",
    "    # Tokenize and generate the response with the original model\n",
    "    encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input_ids=encoding.input_ids, \n",
    "                             max_length=256,  # Adjust max length as per your model's maximum generation capacity\n",
    "                             pad_token_id=tokenizer.eos_token_id, \n",
    "                             eos_token_id=tokenizer.eos_token_id, \n",
    "                             attention_mask=encoding.attention_mask, \n",
    "                             temperature=0.6, \n",
    "                             top_p=0.7, \n",
    "                             repetition_penalty=1.2, \n",
    "                             num_return_sequences=1)\n",
    "    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract and format original model response\n",
    "    formatted_output = extract_response(text_output, query)\n",
    "\n",
    "    # Print original model response\n",
    "    print(dashline)\n",
    "    print(f'ORIGINAL MODEL RESPONSE:\\n{formatted_output}')\n",
    "    print(dashline)\n",
    "\n",
    "    # Tokenize and generate the response with the PEFT model\n",
    "    peft_encoding = peft_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, \n",
    "                                       max_length=256,  # Adjust max length as per your model's maximum generation capacity\n",
    "                                       pad_token_id=peft_tokenizer.eos_token_id, \n",
    "                                       eos_token_id=peft_tokenizer.eos_token_id, \n",
    "                                       attention_mask=peft_encoding.attention_mask, \n",
    "                                       temperature=0.6, \n",
    "                                       top_p=0.7, \n",
    "                                       repetition_penalty=1.2, \n",
    "                                       num_return_sequences=1)\n",
    "    peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract and format PEFT model response\n",
    "    peft_formatted_output = extract_response(peft_text_output, query)\n",
    "\n",
    "    # Print PEFT model response\n",
    "    print(f'PEFT MODEL RESPONSE:\\n{peft_formatted_output}')\n",
    "    print(dashline)\n",
    "\n",
    "def extract_response(text_output, query):\n",
    "    assistant_tag = \"<ASSISTANT>:\"\n",
    "    if assistant_tag in text_output:\n",
    "        response_part = text_output.split(assistant_tag)[1].strip()\n",
    "        if response_part:\n",
    "            # Check for unwanted <HUMAN> tag in the response and remove it\n",
    "            response_part = response_part.split(\"<HUMAN>:\")[0].strip()\n",
    "            return f\"<HUMAN>: {query}\\n<ASSISTANT>: {response_part}\"\n",
    "\n",
    "    # Default to predefined fallback responses if no valid response is found\n",
    "    if \"Sorry, I don't know the answer to this question.\" in text_output:\n",
    "        return f\"<HUMAN>: {query}\\n<ASSISTANT>: Sorry, I don't know the answer to this question.\"\n",
    "    elif \"Kindly, consult the author.\" in text_output:\n",
    "        return f\"<HUMAN>: {query}\\n<ASSISTANT>: Kindly, consult the author.\"\n",
    "\n",
    "    # If the response is completely missing or unidentifiable\n",
    "    return f\"<HUMAN>: {query}\\n<ASSISTANT>: Sorry, I don't know the answer to this question.\"\n",
    "\n",
    "# Example usage\n",
    "generate_answer(\"What is COP26?\")\n",
    "generate_answer(\"How would climate change affect countries?\")\n",
    "generate_answer(\"How do you mnake your own Deodorant?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Calculating BLEU (its tokenization) & ROUGE Score for first 20 Dialogues in our Dataset  </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "I remember Glasgow hosting COP26 last year, but unfortunately I was out of the loop. What was it about?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It was a conference on climate change.\n",
      "Tokenized Predictions: [['It', 'was', 'a', 'conference', 'on', 'climate', 'change', '.']]\n",
      "Tokenized References: [[['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'The', 'term', '“', 'climate', 'change', '”', 'is', 'often', 'used', 'as', 'if', 'it', 'means', 'the', 'same', 'thing', 'as', 'the', 'term', '“', 'global', 'warming', '”', '.', 'The', 'National', 'Academy', 'of', 'Sciences', 'says', '“', 'climate', 'change', '”', 'is', 'growing', 'in', 'favor', 'of', '“', 'global', 'warming', '”', 'because', 'it', 'helps', 'convey', 'that', 'there', 'are', 'other', 'changes', 'in', 'addition', 'to', 'rising', 'temperatures', '.', 'In', 'fact', ',', '“', 'climate', 'change', '”', 'means', 'major', 'changes', 'in', 'temperature', ',', 'rainfall', ',', 'snow', ',', 'or', 'wind', 'patterns', 'lasting', 'for', 'decades', 'or', 'longer', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 4.181379769687496e-160\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342), mid=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342), high=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342)), 'rouge2': AggregateScore(low=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261), mid=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261), high=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261)), 'rougeL': AggregateScore(low=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), mid=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), high=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255)), 'rougeLsum': AggregateScore(low=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), mid=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), high=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The COP26 event is a global united Nations summit about climate change and how countries are planning to tackle it. Name a conference that doesn't have to say something important every single day? No, they aren't all equal. Climate change is a big problem. It affects all members of the human race and any living thing on our wonderful planet. Since 1985, the United Nations has gathered at least once annually for the UNFCCC COP. There have been 26 conferences total. The biggest issue: money. Developing nations are only recently beginning to talk about carbon neutrality around the turn of the century. For many years, the U.S. was one of the largest roadblocks in the way of international cooperation to fight climate change. President Donald Trump withdrew from the Paris Agreement in 2017, making the U.S\n",
      "Tokenized Predictions: [['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'Name', 'a', 'conference', 'that', 'does', \"n't\", 'have', 'to', 'say', 'something', 'important', 'every', 'single', 'day', '?', 'No', ',', 'they', 'are', \"n't\", 'all', 'equal', '.', 'Climate', 'change', 'is', 'a', 'big', 'problem', '.', 'It', 'affects', 'all', 'members', 'of', 'the', 'human', 'race', 'and', 'any', 'living', 'thing', 'on', 'our', 'wonderful', 'planet', '.', 'Since', '1985', ',', 'the', 'United', 'Nations', 'has', 'gathered', 'at', 'least', 'once', 'annually', 'for', 'the', 'UNFCCC', 'COP', '.', 'There', 'have', 'been', '26', 'conferences', 'total', '.', 'The', 'biggest', 'issue', ':', 'money', '.', 'Developing', 'nations', 'are', 'only', 'recently', 'beginning', 'to', 'talk', 'about', 'carbon', 'neutrality', 'around', 'the', 'turn', 'of', 'the', 'century', '.', 'For', 'many', 'years', ',', 'the', 'U.S.', 'was', 'one', 'of', 'the', 'largest', 'roadblocks', 'in', 'the', 'way', 'of', 'international', 'cooperation', 'to', 'fight', 'climate', 'change', '.', 'President', 'Donald', 'Trump', 'withdrew', 'from', 'the', 'Paris', 'Agreement', 'in', '2017', ',', 'making', 'the', 'U.S']]\n",
      "Tokenized References: [[['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'The', 'term', '“', 'climate', 'change', '”', 'is', 'often', 'used', 'as', 'if', 'it', 'means', 'the', 'same', 'thing', 'as', 'the', 'term', '“', 'global', 'warming', '”', '.', 'The', 'National', 'Academy', 'of', 'Sciences', 'says', '“', 'climate', 'change', '”', 'is', 'growing', 'in', 'favor', 'of', '“', 'global', 'warming', '”', 'because', 'it', 'helps', 'convey', 'that', 'there', 'are', 'other', 'changes', 'in', 'addition', 'to', 'rising', 'temperatures', '.', 'In', 'fact', ',', '“', 'climate', 'change', '”', 'means', 'major', 'changes', 'in', 'temperature', ',', 'rainfall', ',', 'snow', ',', 'or', 'wind', 'patterns', 'lasting', 'for', 'decades', 'or', 'longer', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.15833413710075372\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675), mid=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675), high=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675)), 'rouge2': AggregateScore(low=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557), mid=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557), high=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557)), 'rougeL': AggregateScore(low=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), mid=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), high=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634)), 'rougeLsum': AggregateScore(low=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), mid=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), high=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Interesting. What are the effects of these changes?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "The effects are as follows:\n",
      "Tokenized Predictions: [['The', 'effects', 'are', 'as', 'follows', ':']]\n",
      "Tokenized References: [[['Climate', 'change', 'is', 'very', 'likely', 'having', 'an', 'impact', 'now', 'on', 'our', 'planet', 'and', 'its', 'life', ',', 'according', 'to', 'the', 'latest', 'instalment', 'of', 'a', 'report', 'published', 'by', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', '(', 'IPCC', ')', '.', 'And', 'the', 'future', 'problems', 'caused', 'by', 'rising', 'seas', ',', 'growing', 'deserts', ',', 'and', 'more', 'frequent', 'droughts', 'all', 'look', 'set', 'to', 'affect', 'the', 'developing', 'world', 'more', 'than', 'rich', 'countries', ',', 'they', 'add', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 0\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077)), 'rougeLsum': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Climate change is a huge problem that affects the planet and all its life. Here on Earth, it causes problems such as rising sea levels, changing weather patterns and the extinction of species. Worse still, climate change also caused the Antarctic ice sheet to collapse, which made the ocean water more salty and acidic. In addition, it affected the Gulf Stream, which was transporting warm waters from Mexico into Europe. This slowed down or stopped completely resulting in cold temperatures across much of the continent. The future looks even bleaker as the melting Arctic ice will lead to longer and deeper temperature dips. As the ice melts, dark Arctic waters will absorb more heat, causing further ice melt. A never-ending cycle unless we find a way to break it. The world’s leading scientists now agree that climate change is not only a scientific fact, but it is having – and will continue to have – a serious impact\n",
      "Tokenized Predictions: [['Climate', 'change', 'is', 'a', 'huge', 'problem', 'that', 'affects', 'the', 'planet', 'and', 'all', 'its', 'life', '.', 'Here', 'on', 'Earth', ',', 'it', 'causes', 'problems', 'such', 'as', 'rising', 'sea', 'levels', ',', 'changing', 'weather', 'patterns', 'and', 'the', 'extinction', 'of', 'species', '.', 'Worse', 'still', ',', 'climate', 'change', 'also', 'caused', 'the', 'Antarctic', 'ice', 'sheet', 'to', 'collapse', ',', 'which', 'made', 'the', 'ocean', 'water', 'more', 'salty', 'and', 'acidic', '.', 'In', 'addition', ',', 'it', 'affected', 'the', 'Gulf', 'Stream', ',', 'which', 'was', 'transporting', 'warm', 'waters', 'from', 'Mexico', 'into', 'Europe', '.', 'This', 'slowed', 'down', 'or', 'stopped', 'completely', 'resulting', 'in', 'cold', 'temperatures', 'across', 'much', 'of', 'the', 'continent', '.', 'The', 'future', 'looks', 'even', 'bleaker', 'as', 'the', 'melting', 'Arctic', 'ice', 'will', 'lead', 'to', 'longer', 'and', 'deeper', 'temperature', 'dips', '.', 'As', 'the', 'ice', 'melts', ',', 'dark', 'Arctic', 'waters', 'will', 'absorb', 'more', 'heat', ',', 'causing', 'further', 'ice', 'melt', '.', 'A', 'never-ending', 'cycle', 'unless', 'we', 'find', 'a', 'way', 'to', 'break', 'it', '.', 'The', 'world', '’', 's', 'leading', 'scientists', 'now', 'agree', 'that', 'climate', 'change', 'is', 'not', 'only', 'a', 'scientific', 'fact', ',', 'but', 'it', 'is', 'having', '–', 'and', 'will', 'continue', 'to', 'have', '–', 'a', 'serious', 'impact']]\n",
      "Tokenized References: [[['Climate', 'change', 'is', 'very', 'likely', 'having', 'an', 'impact', 'now', 'on', 'our', 'planet', 'and', 'its', 'life', ',', 'according', 'to', 'the', 'latest', 'instalment', 'of', 'a', 'report', 'published', 'by', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', '(', 'IPCC', ')', '.', 'And', 'the', 'future', 'problems', 'caused', 'by', 'rising', 'seas', ',', 'growing', 'deserts', ',', 'and', 'more', 'frequent', 'droughts', 'all', 'look', 'set', 'to', 'affect', 'the', 'developing', 'world', 'more', 'than', 'rich', 'countries', ',', 'they', 'add', '.']]]\n",
      "PEFT MODEL BLEU Score: 8.631570841076429e-79\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706), mid=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706), high=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706)), 'rouge2': AggregateScore(low=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481), mid=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481), high=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481)), 'rougeL': AggregateScore(low=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), mid=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), high=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046)), 'rougeLsum': AggregateScore(low=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), mid=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), high=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s rather vague. Can you be more specific?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "I’m sorry, I don’t know the answer to this question.\n",
      "Tokenized Predictions: [['I', '’', 'm', 'sorry', ',', 'I', 'don', '’', 't', 'know', 'the', 'answer', 'to', 'this', 'question', '.']]\n",
      "Tokenized References: [[['Climate', 'change', 'is', 'impacting', 'human', 'health', 'in', 'countless', 'ways', ',', 'but', 'four', 'are', 'worth', 'emphasizing', '.', 'First', ',', 'pollution', 'from', 'burning', 'fossil', 'fuels', 'is', 'bad', 'enough', 'for', 'the', 'air', 'we', 'breathe', ',', 'but', 'many', 'impacts', 'of', 'climate', 'change', 'also', 'impact', 'air', 'quality', '.', 'Second', ',', 'as', 'our', 'climate', 'becomes', 'warmer', ',', 'some', 'insects', 'will', 'see', 'their', 'geographic', 'ranges', 'grow', '–', 'bringing', 'the', 'Lyme', 'disease', 'and', 'West', 'Nile', 'or', 'Zika', 'viruses', 'they', 'carry', 'along', 'with', 'them', 'to', 'new', 'regions', '.', 'Third', ',', 'melting', 'ice', 'caps', ',', 'ice', 'sheets', 'and', 'glaciers', ',', 'as', 'well', 'as', 'expanding', 'warming', 'waters', ',', 'will', 'lead', 'to', 'rising', 'sea', 'levels', '.', 'Rising', 'sea', 'levels', 'could', 'push', 'saltwater', 'into', 'freshwater', 'aquifers', ',', 'making', 'the', 'water', 'unusable', 'for', 'drinking', 'or', 'irrigation', '.', 'Finally', ',', 'water', 'pollution', 'can', 'have', 'a', 'multitude', 'of', 'negative', 'effects', 'on', 'the', 'environment', ',', 'some', 'of', 'which', 'can', 'lead', 'to', 'even', 'more', 'problems', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 3.365610469820687e-235\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134), mid=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134), high=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134), mid=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134), high=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134)), 'rougeLsum': AggregateScore(low=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134), mid=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134), high=Score(precision=0.16666666666666666, recall=0.015503875968992248, fmeasure=0.028368794326241134))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "To raise children successfully, here are the four types of parenting styles that you can try: Authoritarian: This type of parent is present always when a child needs support or guidance. He never lets any mistake go unattended. Is constantly on their throats. Always criticizes and shows no sign of affection. Avoid this one at all costs! It will only make your kid resentful towards you. Permissive: As a parent, you are their friend first and foremost. You let them do whatever they want, without much regard for their morals and values. They grow up feeling estranged from you. Do this at your own peril. This kind of parent creates spoiled, arrogant adults. The second kind of parenting style is attachment based. Here, the relationship between a parent and a child is very important. Dr. Barry Weinstein explains this in his book ‘The Attachment Parenting Book’. He says\n",
      "Tokenized Predictions: [['To', 'raise', 'children', 'successfully', ',', 'here', 'are', 'the', 'four', 'types', 'of', 'parenting', 'styles', 'that', 'you', 'can', 'try', ':', 'Authoritarian', ':', 'This', 'type', 'of', 'parent', 'is', 'present', 'always', 'when', 'a', 'child', 'needs', 'support', 'or', 'guidance', '.', 'He', 'never', 'lets', 'any', 'mistake', 'go', 'unattended', '.', 'Is', 'constantly', 'on', 'their', 'throats', '.', 'Always', 'criticizes', 'and', 'shows', 'no', 'sign', 'of', 'affection', '.', 'Avoid', 'this', 'one', 'at', 'all', 'costs', '!', 'It', 'will', 'only', 'make', 'your', 'kid', 'resentful', 'towards', 'you', '.', 'Permissive', ':', 'As', 'a', 'parent', ',', 'you', 'are', 'their', 'friend', 'first', 'and', 'foremost', '.', 'You', 'let', 'them', 'do', 'whatever', 'they', 'want', ',', 'without', 'much', 'regard', 'for', 'their', 'morals', 'and', 'values', '.', 'They', 'grow', 'up', 'feeling', 'estranged', 'from', 'you', '.', 'Do', 'this', 'at', 'your', 'own', 'peril', '.', 'This', 'kind', 'of', 'parent', 'creates', 'spoiled', ',', 'arrogant', 'adults', '.', 'The', 'second', 'kind', 'of', 'parenting', 'style', 'is', 'attachment', 'based', '.', 'Here', ',', 'the', 'relationship', 'between', 'a', 'parent', 'and', 'a', 'child', 'is', 'very', 'important', '.', 'Dr.', 'Barry', 'Weinstein', 'explains', 'this', 'in', 'his', 'book', '‘', 'The', 'Attachment', 'Parenting', 'Book', '’', '.', 'He', 'says']]\n",
      "Tokenized References: [[['Climate', 'change', 'is', 'impacting', 'human', 'health', 'in', 'countless', 'ways', ',', 'but', 'four', 'are', 'worth', 'emphasizing', '.', 'First', ',', 'pollution', 'from', 'burning', 'fossil', 'fuels', 'is', 'bad', 'enough', 'for', 'the', 'air', 'we', 'breathe', ',', 'but', 'many', 'impacts', 'of', 'climate', 'change', 'also', 'impact', 'air', 'quality', '.', 'Second', ',', 'as', 'our', 'climate', 'becomes', 'warmer', ',', 'some', 'insects', 'will', 'see', 'their', 'geographic', 'ranges', 'grow', '–', 'bringing', 'the', 'Lyme', 'disease', 'and', 'West', 'Nile', 'or', 'Zika', 'viruses', 'they', 'carry', 'along', 'with', 'them', 'to', 'new', 'regions', '.', 'Third', ',', 'melting', 'ice', 'caps', ',', 'ice', 'sheets', 'and', 'glaciers', ',', 'as', 'well', 'as', 'expanding', 'warming', 'waters', ',', 'will', 'lead', 'to', 'rising', 'sea', 'levels', '.', 'Rising', 'sea', 'levels', 'could', 'push', 'saltwater', 'into', 'freshwater', 'aquifers', ',', 'making', 'the', 'water', 'unusable', 'for', 'drinking', 'or', 'irrigation', '.', 'Finally', ',', 'water', 'pollution', 'can', 'have', 'a', 'multitude', 'of', 'negative', 'effects', 'on', 'the', 'environment', ',', 'some', 'of', 'which', 'can', 'lead', 'to', 'even', 'more', 'problems', '.']]]\n",
      "PEFT MODEL BLEU Score: 1.2147752671518514e-231\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.19594594594594594, recall=0.2248062015503876, fmeasure=0.20938628158844763), mid=Score(precision=0.19594594594594594, recall=0.2248062015503876, fmeasure=0.20938628158844763), high=Score(precision=0.19594594594594594, recall=0.2248062015503876, fmeasure=0.20938628158844763)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.07432432432432433, recall=0.08527131782945736, fmeasure=0.07942238267148015), mid=Score(precision=0.07432432432432433, recall=0.08527131782945736, fmeasure=0.07942238267148015), high=Score(precision=0.07432432432432433, recall=0.08527131782945736, fmeasure=0.07942238267148015)), 'rougeLsum': AggregateScore(low=Score(precision=0.07432432432432433, recall=0.08527131782945736, fmeasure=0.07942238267148015), mid=Score(precision=0.07432432432432433, recall=0.08527131782945736, fmeasure=0.07942238267148015), high=Score(precision=0.07432432432432433, recall=0.08527131782945736, fmeasure=0.07942238267148015))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Woah. They’re not all bad, right?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "No, they’re not.\n",
      "Tokenized Predictions: [['No', ',', 'they', '’', 're', 'not', '.']]\n",
      "Tokenized References: [[['A', 'changing', 'climate', 'is', \"n't\", 'inherently', 'bad', ',', 'and', 'commentators', 'who', 'are', 'skeptical', 'about', 'the', 'risks', 'posed', 'by', 'global', 'warming', 'often', 'point', 'to', 'the', 'benefits', 'that', 'higher', 'temperatures', 'could', 'bring', '–', 'such', 'as', 'longer', 'growing', 'seasons', 'in', 'cool', 'countries', 'and', 'more', 'efficient', 'shipping', 'routes', 'through', 'an', 'ice-free', 'Arctic', '.', 'In', 'fact', ',', 'Professor', 'Richard', 'Tol', 'of', 'Sussex', 'University', 'reviewed', '14', 'different', 'studies', 'of', 'future', 'climate', 'trends', 'and', 'found', 'that', 'the', 'overall', 'aggregate', 'effects', 'of', 'climate', 'change', 'are', 'positive', 'today', '—', 'and', 'likely', 'to', 'stay', 'positive', 'until', 'around', '2080', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 1.0890345730713926e-236\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeLsum': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "A solid pitching model from Nature and a few other sources pitches perfectly every time: The Pitching Perfection book by Dave Kindig. This book covers everything from proper stance to delivery of pitches and uses data on athletes to prove that perfect pitch isn't necessary. Another great source is Physics Today (1990). According to this article, perfect pitchers may throw slightly less efficient pitches but it saves their arms. Hockey players need little else than good posture and consistent ball speed and trajectory for hockey pucks to work. If you try to make a nearly-perfect pellet-like pitch, the cost is poor consistency. For most people, good pitches aren't costing them any extra accuracy.  Gizmodo UK. How Can You Make Your Pitch More Positive? Maybe It's Time To Change Your Phone! By Ash Williams Feb 26, 2016 at 16:30 ET\n",
      "Tokenized Predictions: [['A', 'solid', 'pitching', 'model', 'from', 'Nature', 'and', 'a', 'few', 'other', 'sources', 'pitches', 'perfectly', 'every', 'time', ':', 'The', 'Pitching', 'Perfection', 'book', 'by', 'Dave', 'Kindig', '.', 'This', 'book', 'covers', 'everything', 'from', 'proper', 'stance', 'to', 'delivery', 'of', 'pitches', 'and', 'uses', 'data', 'on', 'athletes', 'to', 'prove', 'that', 'perfect', 'pitch', 'is', \"n't\", 'necessary', '.', 'Another', 'great', 'source', 'is', 'Physics', 'Today', '(', '1990', ')', '.', 'According', 'to', 'this', 'article', ',', 'perfect', 'pitchers', 'may', 'throw', 'slightly', 'less', 'efficient', 'pitches', 'but', 'it', 'saves', 'their', 'arms', '.', 'Hockey', 'players', 'need', 'little', 'else', 'than', 'good', 'posture', 'and', 'consistent', 'ball', 'speed', 'and', 'trajectory', 'for', 'hockey', 'pucks', 'to', 'work', '.', 'If', 'you', 'try', 'to', 'make', 'a', 'nearly-perfect', 'pellet-like', 'pitch', ',', 'the', 'cost', 'is', 'poor', 'consistency', '.', 'For', 'most', 'people', ',', 'good', 'pitches', 'are', \"n't\", 'costing', 'them', 'any', 'extra', 'accuracy', '.', 'Gizmodo', 'UK', '.', 'How', 'Can', 'You', 'Make', 'Your', 'Pitch', 'More', 'Positive', '?', 'Maybe', 'It', \"'s\", 'Time', 'To', 'Change', 'Your', 'Phone', '!', 'By', 'Ash', 'Williams', 'Feb', '26', ',', '2016', 'at', '16:30', 'ET']]\n",
      "Tokenized References: [[['A', 'changing', 'climate', 'is', \"n't\", 'inherently', 'bad', ',', 'and', 'commentators', 'who', 'are', 'skeptical', 'about', 'the', 'risks', 'posed', 'by', 'global', 'warming', 'often', 'point', 'to', 'the', 'benefits', 'that', 'higher', 'temperatures', 'could', 'bring', '–', 'such', 'as', 'longer', 'growing', 'seasons', 'in', 'cool', 'countries', 'and', 'more', 'efficient', 'shipping', 'routes', 'through', 'an', 'ice-free', 'Arctic', '.', 'In', 'fact', ',', 'Professor', 'Richard', 'Tol', 'of', 'Sussex', 'University', 'reviewed', '14', 'different', 'studies', 'of', 'future', 'climate', 'trends', 'and', 'found', 'that', 'the', 'overall', 'aggregate', 'effects', 'of', 'climate', 'change', 'are', 'positive', 'today', '—', 'and', 'likely', 'to', 'stay', 'positive', 'until', 'around', '2080', '.']]]\n",
      "PEFT MODEL BLEU Score: 2.473700193994718e-155\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.1310344827586207, recall=0.2261904761904762, fmeasure=0.165938864628821), mid=Score(precision=0.1310344827586207, recall=0.2261904761904762, fmeasure=0.165938864628821), high=Score(precision=0.1310344827586207, recall=0.2261904761904762, fmeasure=0.165938864628821)), 'rouge2': AggregateScore(low=Score(precision=0.006944444444444444, recall=0.012048192771084338, fmeasure=0.008810572687224669), mid=Score(precision=0.006944444444444444, recall=0.012048192771084338, fmeasure=0.008810572687224669), high=Score(precision=0.006944444444444444, recall=0.012048192771084338, fmeasure=0.008810572687224669)), 'rougeL': AggregateScore(low=Score(precision=0.08275862068965517, recall=0.14285714285714285, fmeasure=0.10480349344978165), mid=Score(precision=0.08275862068965517, recall=0.14285714285714285, fmeasure=0.10480349344978165), high=Score(precision=0.08275862068965517, recall=0.14285714285714285, fmeasure=0.10480349344978165)), 'rougeLsum': AggregateScore(low=Score(precision=0.08275862068965517, recall=0.14285714285714285, fmeasure=0.10480349344978165), mid=Score(precision=0.08275862068965517, recall=0.14285714285714285, fmeasure=0.10480349344978165), high=Score(precision=0.08275862068965517, recall=0.14285714285714285, fmeasure=0.10480349344978165))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "I remember Glasgow hosting COP26 last year, but unfortunately I was out of the loop. What was it about?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It was a conference on climate change.\n",
      "Tokenized Predictions: [['It', 'was', 'a', 'conference', 'on', 'climate', 'change', '.']]\n",
      "Tokenized References: [[['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'The', 'term', '“', 'climate', 'change', '”', 'is', 'often', 'used', 'as', 'if', 'it', 'means', 'the', 'same', 'thing', 'as', 'the', 'term', '“', 'global', 'warming', '”', '.', 'The', 'National', 'Academy', 'of', 'Sciences', 'says', '“', 'climate', 'change', '”', 'is', 'growing', 'in', 'favor', 'of', '“', 'global', 'warming', '”', 'because', 'it', 'helps', 'convey', 'that', 'there', 'are', 'other', 'changes', 'in', 'addition', 'to', 'rising', 'temperatures', '.', 'In', 'fact', ',', '“', 'climate', 'change', '”', 'means', 'major', 'changes', 'in', 'temperature', ',', 'rainfall', ',', 'snow', ',', 'or', 'wind', 'patterns', 'lasting', 'for', 'decades', 'or', 'longer', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 4.181379769687496e-160\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342), mid=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342), high=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342)), 'rouge2': AggregateScore(low=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261), mid=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261), high=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261)), 'rougeL': AggregateScore(low=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), mid=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), high=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255)), 'rougeLsum': AggregateScore(low=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), mid=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), high=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The COP26 event is a global united Nations summit about climate change and how countries are planning to tackle it. Name a conference that doesn't have to say something important every single day? No, they aren't all equal. Climate change is a big problem. It affects all members of the human race and any living thing on our wonderful planet. Since 1985, the United Nations has gathered at least once annually for the UNFCCC COP. There have been 26 conferences total. The biggest issue: money. Developing nations are only recently beginning to talk about carbon neutrality around the turn of the century. For many years, the U.S. was one of the largest roadblocks in the way of international cooperation to fight climate change. President Donald Trump withdrew from the Paris Agreement in 2017, making the U.S\n",
      "Tokenized Predictions: [['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'Name', 'a', 'conference', 'that', 'does', \"n't\", 'have', 'to', 'say', 'something', 'important', 'every', 'single', 'day', '?', 'No', ',', 'they', 'are', \"n't\", 'all', 'equal', '.', 'Climate', 'change', 'is', 'a', 'big', 'problem', '.', 'It', 'affects', 'all', 'members', 'of', 'the', 'human', 'race', 'and', 'any', 'living', 'thing', 'on', 'our', 'wonderful', 'planet', '.', 'Since', '1985', ',', 'the', 'United', 'Nations', 'has', 'gathered', 'at', 'least', 'once', 'annually', 'for', 'the', 'UNFCCC', 'COP', '.', 'There', 'have', 'been', '26', 'conferences', 'total', '.', 'The', 'biggest', 'issue', ':', 'money', '.', 'Developing', 'nations', 'are', 'only', 'recently', 'beginning', 'to', 'talk', 'about', 'carbon', 'neutrality', 'around', 'the', 'turn', 'of', 'the', 'century', '.', 'For', 'many', 'years', ',', 'the', 'U.S.', 'was', 'one', 'of', 'the', 'largest', 'roadblocks', 'in', 'the', 'way', 'of', 'international', 'cooperation', 'to', 'fight', 'climate', 'change', '.', 'President', 'Donald', 'Trump', 'withdrew', 'from', 'the', 'Paris', 'Agreement', 'in', '2017', ',', 'making', 'the', 'U.S']]\n",
      "Tokenized References: [[['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'The', 'term', '“', 'climate', 'change', '”', 'is', 'often', 'used', 'as', 'if', 'it', 'means', 'the', 'same', 'thing', 'as', 'the', 'term', '“', 'global', 'warming', '”', '.', 'The', 'National', 'Academy', 'of', 'Sciences', 'says', '“', 'climate', 'change', '”', 'is', 'growing', 'in', 'favor', 'of', '“', 'global', 'warming', '”', 'because', 'it', 'helps', 'convey', 'that', 'there', 'are', 'other', 'changes', 'in', 'addition', 'to', 'rising', 'temperatures', '.', 'In', 'fact', ',', '“', 'climate', 'change', '”', 'means', 'major', 'changes', 'in', 'temperature', ',', 'rainfall', ',', 'snow', ',', 'or', 'wind', 'patterns', 'lasting', 'for', 'decades', 'or', 'longer', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.15833413710075372\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675), mid=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675), high=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675)), 'rouge2': AggregateScore(low=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557), mid=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557), high=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557)), 'rougeL': AggregateScore(low=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), mid=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), high=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634)), 'rougeLsum': AggregateScore(low=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), mid=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), high=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Interesting. What are the effects of these changes?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "The effects are as follows:\n",
      "Tokenized Predictions: [['The', 'effects', 'are', 'as', 'follows', ':']]\n",
      "Tokenized References: [[['Climate', 'change', 'is', 'very', 'likely', 'having', 'an', 'impact', 'now', 'on', 'our', 'planet', 'and', 'its', 'life', ',', 'according', 'to', 'the', 'latest', 'instalment', 'of', 'a', 'report', 'published', 'by', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', '(', 'IPCC', ')', '.', 'And', 'the', 'future', 'problems', 'caused', 'by', 'rising', 'seas', ',', 'growing', 'deserts', ',', 'and', 'more', 'frequent', 'droughts', 'all', 'look', 'set', 'to', 'affect', 'the', 'developing', 'world', 'more', 'than', 'rich', 'countries', ',', 'they', 'add', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 0\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077)), 'rougeLsum': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Climate change is a huge problem that affects the planet and all its life. Here on Earth, it causes problems such as rising sea levels, changing weather patterns and the extinction of species. Worse still, climate change also caused the Antarctic ice sheet to collapse, which made the ocean water more salty and acidic. In addition, it affected the Gulf Stream, which was transporting warm waters from Mexico into Europe. This slowed down or stopped completely resulting in cold temperatures across much of the continent. The future looks even bleaker as the melting Arctic ice will lead to longer and deeper temperature dips. As the ice melts, dark Arctic waters will absorb more heat, causing further ice melt. A never-ending cycle unless we find a way to break it. The world’s leading scientists now agree that climate change is not only a scientific fact, but it is having – and will continue to have – a serious impact\n",
      "Tokenized Predictions: [['Climate', 'change', 'is', 'a', 'huge', 'problem', 'that', 'affects', 'the', 'planet', 'and', 'all', 'its', 'life', '.', 'Here', 'on', 'Earth', ',', 'it', 'causes', 'problems', 'such', 'as', 'rising', 'sea', 'levels', ',', 'changing', 'weather', 'patterns', 'and', 'the', 'extinction', 'of', 'species', '.', 'Worse', 'still', ',', 'climate', 'change', 'also', 'caused', 'the', 'Antarctic', 'ice', 'sheet', 'to', 'collapse', ',', 'which', 'made', 'the', 'ocean', 'water', 'more', 'salty', 'and', 'acidic', '.', 'In', 'addition', ',', 'it', 'affected', 'the', 'Gulf', 'Stream', ',', 'which', 'was', 'transporting', 'warm', 'waters', 'from', 'Mexico', 'into', 'Europe', '.', 'This', 'slowed', 'down', 'or', 'stopped', 'completely', 'resulting', 'in', 'cold', 'temperatures', 'across', 'much', 'of', 'the', 'continent', '.', 'The', 'future', 'looks', 'even', 'bleaker', 'as', 'the', 'melting', 'Arctic', 'ice', 'will', 'lead', 'to', 'longer', 'and', 'deeper', 'temperature', 'dips', '.', 'As', 'the', 'ice', 'melts', ',', 'dark', 'Arctic', 'waters', 'will', 'absorb', 'more', 'heat', ',', 'causing', 'further', 'ice', 'melt', '.', 'A', 'never-ending', 'cycle', 'unless', 'we', 'find', 'a', 'way', 'to', 'break', 'it', '.', 'The', 'world', '’', 's', 'leading', 'scientists', 'now', 'agree', 'that', 'climate', 'change', 'is', 'not', 'only', 'a', 'scientific', 'fact', ',', 'but', 'it', 'is', 'having', '–', 'and', 'will', 'continue', 'to', 'have', '–', 'a', 'serious', 'impact']]\n",
      "Tokenized References: [[['Climate', 'change', 'is', 'very', 'likely', 'having', 'an', 'impact', 'now', 'on', 'our', 'planet', 'and', 'its', 'life', ',', 'according', 'to', 'the', 'latest', 'instalment', 'of', 'a', 'report', 'published', 'by', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', '(', 'IPCC', ')', '.', 'And', 'the', 'future', 'problems', 'caused', 'by', 'rising', 'seas', ',', 'growing', 'deserts', ',', 'and', 'more', 'frequent', 'droughts', 'all', 'look', 'set', 'to', 'affect', 'the', 'developing', 'world', 'more', 'than', 'rich', 'countries', ',', 'they', 'add', '.']]]\n",
      "PEFT MODEL BLEU Score: 8.631570841076429e-79\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706), mid=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706), high=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706)), 'rouge2': AggregateScore(low=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481), mid=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481), high=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481)), 'rougeL': AggregateScore(low=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), mid=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), high=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046)), 'rougeLsum': AggregateScore(low=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), mid=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), high=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s interesting. Tell me more.\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "I’m afraid I can’t.\n",
      "Tokenized Predictions: [['I', '’', 'm', 'afraid', 'I', 'can', '’', 't', '.']]\n",
      "Tokenized References: [[['For', 'several', 'years', ',', 'there', 'have', 'been', 'concerns', 'that', 'climate', 'change', 'negotiations', 'will', 'essentially', 'ignore', 'a', 'key', 'principle', 'of', 'climate', 'change', 'negotiation', 'frameworks', ':', 'the', 'common', 'but', 'differentiated', 'responsibilities', '.', 'Realizing', 'that', 'greenhouse', 'emissions', 'remain', 'in', 'the', 'atmosphere', 'for', 'a', 'very', 'long', 'time', ',', 'this', 'principle', 'recognizes', 'that', 'historically', ':', 'Industrialized', 'nations', 'have', 'emitted', 'far', 'more', 'greenhouse', 'gas', 'emissions', '(', 'even', 'if', 'some', 'developing', 'nations', 'are', 'only', 'now', 'increasing', 'theirs', ')', ';', 'Rich', 'countries', ',', 'therefore', ',', 'face', 'the', 'biggest', 'responsibility', 'and', 'burden', 'for', 'action', 'to', 'address', 'climate', 'change', ';', 'and', 'Rich', 'countries', ',', 'therefore', ',', 'must', 'support', 'developing', 'nations', 'adapt—through', 'financing', 'and', 'technology', 'transfer', ',', 'for', 'example', '.', 'This', 'notion', 'of', 'climate', 'justice', 'is', 'typically', 'ignored', 'by', 'many', 'rich', 'nations', 'and', 'their', 'mainstream', 'media', ',', 'making', 'it', 'easy', 'to', 'blame', 'China', ',', 'India', 'and', 'other', 'developing', 'countries', 'for', 'failures', 'in', 'climate', 'change', 'mitigation', 'negotiations', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 2.576443261051062e-238\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeLsum': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "A lot of online activities are tracked in some way; most visitors can be followed in at least two ways: by placing a tracking cookie, which is essentially a unique identifier, and by logging information voluntarily (e.g., through a login), as well as other means such as camera recognition, fingerprinting, and many more. Websites track users for various reasons, mostly for advertising, data mining, and targeting ads. The latter includes both promoting products and manipulating people. Tracking prevents ad blockers from blocking all advertisements. At the same time, advertisers have the option to be excluded from tracking. There are also alternative methods of blocking some trackers but not others. Adblock Plus blocks certain types of trackers but not webfonts, for example. DoEzApps browser extension blocks almost all trackers but allows script and style files to load. Ghostery is another popular ad blocker that does not block all trackers but instead\n",
      "Tokenized Predictions: [['A', 'lot', 'of', 'online', 'activities', 'are', 'tracked', 'in', 'some', 'way', ';', 'most', 'visitors', 'can', 'be', 'followed', 'in', 'at', 'least', 'two', 'ways', ':', 'by', 'placing', 'a', 'tracking', 'cookie', ',', 'which', 'is', 'essentially', 'a', 'unique', 'identifier', ',', 'and', 'by', 'logging', 'information', 'voluntarily', '(', 'e.g.', ',', 'through', 'a', 'login', ')', ',', 'as', 'well', 'as', 'other', 'means', 'such', 'as', 'camera', 'recognition', ',', 'fingerprinting', ',', 'and', 'many', 'more', '.', 'Websites', 'track', 'users', 'for', 'various', 'reasons', ',', 'mostly', 'for', 'advertising', ',', 'data', 'mining', ',', 'and', 'targeting', 'ads', '.', 'The', 'latter', 'includes', 'both', 'promoting', 'products', 'and', 'manipulating', 'people', '.', 'Tracking', 'prevents', 'ad', 'blockers', 'from', 'blocking', 'all', 'advertisements', '.', 'At', 'the', 'same', 'time', ',', 'advertisers', 'have', 'the', 'option', 'to', 'be', 'excluded', 'from', 'tracking', '.', 'There', 'are', 'also', 'alternative', 'methods', 'of', 'blocking', 'some', 'trackers', 'but', 'not', 'others', '.', 'Adblock', 'Plus', 'blocks', 'certain', 'types', 'of', 'trackers', 'but', 'not', 'webfonts', ',', 'for', 'example', '.', 'DoEzApps', 'browser', 'extension', 'blocks', 'almost', 'all', 'trackers', 'but', 'allows', 'script', 'and', 'style', 'files', 'to', 'load', '.', 'Ghostery', 'is', 'another', 'popular', 'ad', 'blocker', 'that', 'does', 'not', 'block', 'all', 'trackers', 'but', 'instead']]\n",
      "Tokenized References: [[['For', 'several', 'years', ',', 'there', 'have', 'been', 'concerns', 'that', 'climate', 'change', 'negotiations', 'will', 'essentially', 'ignore', 'a', 'key', 'principle', 'of', 'climate', 'change', 'negotiation', 'frameworks', ':', 'the', 'common', 'but', 'differentiated', 'responsibilities', '.', 'Realizing', 'that', 'greenhouse', 'emissions', 'remain', 'in', 'the', 'atmosphere', 'for', 'a', 'very', 'long', 'time', ',', 'this', 'principle', 'recognizes', 'that', 'historically', ':', 'Industrialized', 'nations', 'have', 'emitted', 'far', 'more', 'greenhouse', 'gas', 'emissions', '(', 'even', 'if', 'some', 'developing', 'nations', 'are', 'only', 'now', 'increasing', 'theirs', ')', ';', 'Rich', 'countries', ',', 'therefore', ',', 'face', 'the', 'biggest', 'responsibility', 'and', 'burden', 'for', 'action', 'to', 'address', 'climate', 'change', ';', 'and', 'Rich', 'countries', ',', 'therefore', ',', 'must', 'support', 'developing', 'nations', 'adapt—through', 'financing', 'and', 'technology', 'transfer', ',', 'for', 'example', '.', 'This', 'notion', 'of', 'climate', 'justice', 'is', 'typically', 'ignored', 'by', 'many', 'rich', 'nations', 'and', 'their', 'mainstream', 'media', ',', 'making', 'it', 'easy', 'to', 'blame', 'China', ',', 'India', 'and', 'other', 'developing', 'countries', 'for', 'failures', 'in', 'climate', 'change', 'mitigation', 'negotiations', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.025676869023137934\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2251655629139073, recall=0.26356589147286824, fmeasure=0.24285714285714285), mid=Score(precision=0.2251655629139073, recall=0.26356589147286824, fmeasure=0.24285714285714285), high=Score(precision=0.2251655629139073, recall=0.26356589147286824, fmeasure=0.24285714285714285)), 'rouge2': AggregateScore(low=Score(precision=0.006666666666666667, recall=0.0078125, fmeasure=0.007194244604316546), mid=Score(precision=0.006666666666666667, recall=0.0078125, fmeasure=0.007194244604316546), high=Score(precision=0.006666666666666667, recall=0.0078125, fmeasure=0.007194244604316546)), 'rougeL': AggregateScore(low=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572), mid=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572), high=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572)), 'rougeLsum': AggregateScore(low=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572), mid=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572), high=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Okay, but how does it affect developing countries?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It doesn't.\n",
      "Tokenized Predictions: [['It', 'does', \"n't\", '.']]\n",
      "Tokenized References: [[['Developing', 'countries', 'are', 'the', 'least', 'able', 'to', 'adapt', 'to', 'climate', 'change', '(', 'and', 'therefore', 'called', '``', 'highly', 'climate', 'vulnerable', \"''\", ')', 'due', 'to', 'their', 'low', 'levels', 'of', 'wealth', ',', 'technology', ',', 'education', ',', 'and', 'access', 'to', 'resources', '.', 'Climate', 'change', 'causes', '400,000', 'deaths', 'on', 'average', 'each', 'year', ',', 'mainly', 'due', 'to', 'hunger', 'and', 'diseases', 'in', 'developing', 'countries', '.', 'Interestingly', ',', 'many', 'people', 'in', 'developing', 'nations', 'who', 'had', 'never', 'been', 'educated', 'about', 'climate', 'change', 'have', 'noticed', 'local', 'changes', 'in', 'temperature', 'and', 'precipitation', 'patterns', 'according', 'to', 'a', 'study', 'published', 'in', 'the', 'journal', 'Nature', 'Climate', 'Change', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 2.1795532445072597e-241\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeLsum': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Developing countries are the least able to adapt to climate change (and therefore called \"highly climate vulnerable\") due to their low levels of wealth, technology, education, and access to resources. Climate change causes 400,000 deaths on average each year, mainly in developing nations, largely due to hunger and diseases in impoverished communities. As the temperature continues to rise, rising seas will swallow more and more of the land, forcing population shifts in the days of increased coastal migration. At the same time, ice that once protects against these changes will melt, leading to longer periods of global freezing. The waves will get worse; every one years severe weather events caused by stalled fronts or cyclones will reach the Earth’s coldest surfaces. Furthermore, as the polar caps melt during the winter season, less sunlight will reach the earth causing colder temperatures for a longer period. In addition, the frozen sea will\n",
      "Tokenized Predictions: [['Developing', 'countries', 'are', 'the', 'least', 'able', 'to', 'adapt', 'to', 'climate', 'change', '(', 'and', 'therefore', 'called', '``', 'highly', 'climate', 'vulnerable', \"''\", ')', 'due', 'to', 'their', 'low', 'levels', 'of', 'wealth', ',', 'technology', ',', 'education', ',', 'and', 'access', 'to', 'resources', '.', 'Climate', 'change', 'causes', '400,000', 'deaths', 'on', 'average', 'each', 'year', ',', 'mainly', 'in', 'developing', 'nations', ',', 'largely', 'due', 'to', 'hunger', 'and', 'diseases', 'in', 'impoverished', 'communities', '.', 'As', 'the', 'temperature', 'continues', 'to', 'rise', ',', 'rising', 'seas', 'will', 'swallow', 'more', 'and', 'more', 'of', 'the', 'land', ',', 'forcing', 'population', 'shifts', 'in', 'the', 'days', 'of', 'increased', 'coastal', 'migration', '.', 'At', 'the', 'same', 'time', ',', 'ice', 'that', 'once', 'protects', 'against', 'these', 'changes', 'will', 'melt', ',', 'leading', 'to', 'longer', 'periods', 'of', 'global', 'freezing', '.', 'The', 'waves', 'will', 'get', 'worse', ';', 'every', 'one', 'years', 'severe', 'weather', 'events', 'caused', 'by', 'stalled', 'fronts', 'or', 'cyclones', 'will', 'reach', 'the', 'Earth', '’', 's', 'coldest', 'surfaces', '.', 'Furthermore', ',', 'as', 'the', 'polar', 'caps', 'melt', 'during', 'the', 'winter', 'season', ',', 'less', 'sunlight', 'will', 'reach', 'the', 'earth', 'causing', 'colder', 'temperatures', 'for', 'a', 'longer', 'period', '.', 'In', 'addition', ',', 'the', 'frozen', 'sea', 'will']]\n",
      "Tokenized References: [[['Developing', 'countries', 'are', 'the', 'least', 'able', 'to', 'adapt', 'to', 'climate', 'change', '(', 'and', 'therefore', 'called', '``', 'highly', 'climate', 'vulnerable', \"''\", ')', 'due', 'to', 'their', 'low', 'levels', 'of', 'wealth', ',', 'technology', ',', 'education', ',', 'and', 'access', 'to', 'resources', '.', 'Climate', 'change', 'causes', '400,000', 'deaths', 'on', 'average', 'each', 'year', ',', 'mainly', 'due', 'to', 'hunger', 'and', 'diseases', 'in', 'developing', 'countries', '.', 'Interestingly', ',', 'many', 'people', 'in', 'developing', 'nations', 'who', 'had', 'never', 'been', 'educated', 'about', 'climate', 'change', 'have', 'noticed', 'local', 'changes', 'in', 'temperature', 'and', 'precipitation', 'patterns', 'according', 'to', 'a', 'study', 'published', 'in', 'the', 'journal', 'Nature', 'Climate', 'Change', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.32168708779236976\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.3815789473684211, recall=0.6987951807228916, fmeasure=0.4936170212765958), mid=Score(precision=0.3815789473684211, recall=0.6987951807228916, fmeasure=0.4936170212765958), high=Score(precision=0.3815789473684211, recall=0.6987951807228916, fmeasure=0.4936170212765958)), 'rouge2': AggregateScore(low=Score(precision=0.31788079470198677, recall=0.5853658536585366, fmeasure=0.4120171673819743), mid=Score(precision=0.31788079470198677, recall=0.5853658536585366, fmeasure=0.4120171673819743), high=Score(precision=0.31788079470198677, recall=0.5853658536585366, fmeasure=0.4120171673819743)), 'rougeL': AggregateScore(low=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234), mid=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234), high=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234)), 'rougeLsum': AggregateScore(low=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234), mid=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234), high=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "How are developed countries helping with that?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "They are helping by providing financial aid to developing countries.\n",
      "Tokenized Predictions: [['They', 'are', 'helping', 'by', 'providing', 'financial', 'aid', 'to', 'developing', 'countries', '.']]\n",
      "Tokenized References: [[['Well', ',', 'according', 'to', 'the', 'Paris', 'Agreement', ',', 'every', 'two', 'years', ',', 'developed', 'countries', 'are', 'to', 'communicate', 'the', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'they', 'provide', 'to', 'developing', 'countries', 'to', 'mitigate', 'and', 'adapt', 'to', 'climate', 'change', '.', 'Some', 'developing', 'countries', 'also', 'contribute', 'to', 'climate', 'finance', 'too', '.', 'Already', ',', 'Brazil', '’', 's', 'President', 'Dilma', 'Rousseff', 'said', 'the', 'country', 'is', 'considering', 'contributing', 'to', 'climate', 'finance', ',', 'joining', 'other', 'emerging', 'economies', 'like', 'China', ',', 'which', 'pledged', 'to', 'provide', '$', '3.1', 'billion', 'over', 'three', 'years', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 5.56253195093122e-81\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.5, recall=0.0684931506849315, fmeasure=0.12048192771084336), mid=Score(precision=0.5, recall=0.0684931506849315, fmeasure=0.12048192771084336), high=Score(precision=0.5, recall=0.0684931506849315, fmeasure=0.12048192771084336)), 'rouge2': AggregateScore(low=Score(precision=0.2222222222222222, recall=0.027777777777777776, fmeasure=0.04938271604938271), mid=Score(precision=0.2222222222222222, recall=0.027777777777777776, fmeasure=0.04938271604938271), high=Score(precision=0.2222222222222222, recall=0.027777777777777776, fmeasure=0.04938271604938271)), 'rougeL': AggregateScore(low=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469), mid=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469), high=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469)), 'rougeLsum': AggregateScore(low=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469), mid=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469), high=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Well, according to the Paris Agreement, every two years, developed countries are expected to communicate the projected levels of public climate finance they provide to developing countries to mitigate and adapt to climate change. Some developing countries also contribute to climate finance too. Already, Brazil’s President Dilma Rousseff said the country is considering contributing to climate finance, joining other emerging economies like China, which pledged to provide $3.1 billion over three years. The biggest provider historically has been the United States, despite Republican opposition. Other major contributors include European nations, Japan, and the World Bank Group. Climate finance is intended to help developing countries adapt to rising seas, growing deserts caused by global warming, and money to get rid of their carbon pollution. Rich countries will provide all climate finance necessary to develop emerging markets and developing countries – but they do not say what it will be for. Already, the Trump administration is trying\n",
      "Tokenized Predictions: [['Well', ',', 'according', 'to', 'the', 'Paris', 'Agreement', ',', 'every', 'two', 'years', ',', 'developed', 'countries', 'are', 'expected', 'to', 'communicate', 'the', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'they', 'provide', 'to', 'developing', 'countries', 'to', 'mitigate', 'and', 'adapt', 'to', 'climate', 'change', '.', 'Some', 'developing', 'countries', 'also', 'contribute', 'to', 'climate', 'finance', 'too', '.', 'Already', ',', 'Brazil', '’', 's', 'President', 'Dilma', 'Rousseff', 'said', 'the', 'country', 'is', 'considering', 'contributing', 'to', 'climate', 'finance', ',', 'joining', 'other', 'emerging', 'economies', 'like', 'China', ',', 'which', 'pledged', 'to', 'provide', '$', '3.1', 'billion', 'over', 'three', 'years', '.', 'The', 'biggest', 'provider', 'historically', 'has', 'been', 'the', 'United', 'States', ',', 'despite', 'Republican', 'opposition', '.', 'Other', 'major', 'contributors', 'include', 'European', 'nations', ',', 'Japan', ',', 'and', 'the', 'World', 'Bank', 'Group', '.', 'Climate', 'finance', 'is', 'intended', 'to', 'help', 'developing', 'countries', 'adapt', 'to', 'rising', 'seas', ',', 'growing', 'deserts', 'caused', 'by', 'global', 'warming', ',', 'and', 'money', 'to', 'get', 'rid', 'of', 'their', 'carbon', 'pollution', '.', 'Rich', 'countries', 'will', 'provide', 'all', 'climate', 'finance', 'necessary', 'to', 'develop', 'emerging', 'markets', 'and', 'developing', 'countries', '–', 'but', 'they', 'do', 'not', 'say', 'what', 'it', 'will', 'be', 'for', '.', 'Already', ',', 'the', 'Trump', 'administration', 'is', 'trying']]\n",
      "Tokenized References: [[['Well', ',', 'according', 'to', 'the', 'Paris', 'Agreement', ',', 'every', 'two', 'years', ',', 'developed', 'countries', 'are', 'to', 'communicate', 'the', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'they', 'provide', 'to', 'developing', 'countries', 'to', 'mitigate', 'and', 'adapt', 'to', 'climate', 'change', '.', 'Some', 'developing', 'countries', 'also', 'contribute', 'to', 'climate', 'finance', 'too', '.', 'Already', ',', 'Brazil', '’', 's', 'President', 'Dilma', 'Rousseff', 'said', 'the', 'country', 'is', 'considering', 'contributing', 'to', 'climate', 'finance', ',', 'joining', 'other', 'emerging', 'economies', 'like', 'China', ',', 'which', 'pledged', 'to', 'provide', '$', '3.1', 'billion', 'over', 'three', 'years', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.4556715605238871\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), mid=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), high=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384)), 'rouge2': AggregateScore(low=Score(precision=0.45806451612903226, recall=0.9861111111111112, fmeasure=0.6255506607929516), mid=Score(precision=0.45806451612903226, recall=0.9861111111111112, fmeasure=0.6255506607929516), high=Score(precision=0.45806451612903226, recall=0.9861111111111112, fmeasure=0.6255506607929516)), 'rougeL': AggregateScore(low=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), mid=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), high=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384)), 'rougeLsum': AggregateScore(low=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), mid=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), high=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Are they meeting them?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Yes, they are.\n",
      "Tokenized Predictions: [['Yes', ',', 'they', 'are', '.']]\n",
      "Tokenized References: [[['Developed', 'countries', 'will', 'communicate', 'every', 'two', 'years', 'on', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'to', 'be', 'provided', 'to', 'developing', 'countries', '.', 'Developing', 'countries', 'will', 'report', 'on', 'climate', 'finance', 'they', 'will', 'provide', 'voluntarily', '.', 'Developed', 'countries', 'are', 'not', 'obligated', 'to', 'provide', 'climate', 'finance', ',', 'but', 'they', 'are', 'welcome', 'to', 'contribute', 'financing', 'for', 'climate', 'activities', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 5.531511147753189e-159\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), mid=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), high=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754)), 'rouge2': AggregateScore(low=Score(precision=0.5, recall=0.02040816326530612, fmeasure=0.039215686274509796), mid=Score(precision=0.5, recall=0.02040816326530612, fmeasure=0.039215686274509796), high=Score(precision=0.5, recall=0.02040816326530612, fmeasure=0.039215686274509796)), 'rougeL': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), mid=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), high=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754)), 'rougeLsum': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), mid=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), high=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The International Space Station (ISS) is one of two stations which have been or currently are hosted by NASA. The station since its inception has provided a long-term human presence in space and serves as a laboratory for scientific research, demonstration, and technology development. It consists of components that together form an environment where no terrestrial forces are needed for crew members to carry out their tasks. With over 13 trillion cubic metres of air being supplied for breathing and insulation, it surpasses Earth's surface and is by far the largest human-made structure in space. Due to the magnitude of work conducted by the ISS, disagreements arose between nations regarding distribution of financial contributions. There have also been debates about which country she represents the greatest achievement for. This paper seeks to outline the reasons why the National Academy of Sciences concluded that Hall’s statement was false and caused substantial harm. \n",
      "[Asking how it causes harm] How did his\n",
      "Tokenized Predictions: [['The', 'International', 'Space', 'Station', '(', 'ISS', ')', 'is', 'one', 'of', 'two', 'stations', 'which', 'have', 'been', 'or', 'currently', 'are', 'hosted', 'by', 'NASA', '.', 'The', 'station', 'since', 'its', 'inception', 'has', 'provided', 'a', 'long-term', 'human', 'presence', 'in', 'space', 'and', 'serves', 'as', 'a', 'laboratory', 'for', 'scientific', 'research', ',', 'demonstration', ',', 'and', 'technology', 'development', '.', 'It', 'consists', 'of', 'components', 'that', 'together', 'form', 'an', 'environment', 'where', 'no', 'terrestrial', 'forces', 'are', 'needed', 'for', 'crew', 'members', 'to', 'carry', 'out', 'their', 'tasks', '.', 'With', 'over', '13', 'trillion', 'cubic', 'metres', 'of', 'air', 'being', 'supplied', 'for', 'breathing', 'and', 'insulation', ',', 'it', 'surpasses', 'Earth', \"'s\", 'surface', 'and', 'is', 'by', 'far', 'the', 'largest', 'human-made', 'structure', 'in', 'space', '.', 'Due', 'to', 'the', 'magnitude', 'of', 'work', 'conducted', 'by', 'the', 'ISS', ',', 'disagreements', 'arose', 'between', 'nations', 'regarding', 'distribution', 'of', 'financial', 'contributions', '.', 'There', 'have', 'also', 'been', 'debates', 'about', 'which', 'country', 'she', 'represents', 'the', 'greatest', 'achievement', 'for', '.', 'This', 'paper', 'seeks', 'to', 'outline', 'the', 'reasons', 'why', 'the', 'National', 'Academy', 'of', 'Sciences', 'concluded', 'that', 'Hall', '’', 's', 'statement', 'was', 'false', 'and', 'caused', 'substantial', 'harm', '.', '[', 'Asking', 'how', 'it', 'causes', 'harm', ']', 'How', 'did', 'his']]\n",
      "Tokenized References: [[['Developed', 'countries', 'will', 'communicate', 'every', 'two', 'years', 'on', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'to', 'be', 'provided', 'to', 'developing', 'countries', '.', 'Developing', 'countries', 'will', 'report', 'on', 'climate', 'finance', 'they', 'will', 'provide', 'voluntarily', '.', 'Developed', 'countries', 'are', 'not', 'obligated', 'to', 'provide', 'climate', 'finance', ',', 'but', 'they', 'are', 'welcome', 'to', 'contribute', 'financing', 'for', 'climate', 'activities', '.']]]\n",
      "PEFT MODEL BLEU Score: 9.484203087232283e-232\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.05521472392638037, recall=0.18, fmeasure=0.08450704225352111), mid=Score(precision=0.05521472392638037, recall=0.18, fmeasure=0.08450704225352111), high=Score(precision=0.05521472392638037, recall=0.18, fmeasure=0.08450704225352111)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408), mid=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408), high=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408)), 'rougeLsum': AggregateScore(low=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408), mid=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408), high=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s not too relevant to my question. By the way, is that related to last year’s conference?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Yes, it is.\n",
      "Tokenized Predictions: [['Yes', ',', 'it', 'is', '.']]\n",
      "Tokenized References: [[['The', 'Paris', 'Agreement', 'is', 'a', 'big', 'international', 'agreement', 'to', 'tackle', 'climate', 'change', '.', '195', 'countries', 'signed', 'the', 'agreement', 'in', 'December', '2015', 'at', 'the', '21st', 'Conference', 'of', 'the', 'Parties', '(', 'COP21', ')', 'and', '186', 'have', 'become', 'party', 'to', 'it', '.', 'The', 'long-term', 'goal', 'is', 'to', 'keep', 'the', 'global', 'average', 'temperature', 'well', 'below', '2C', 'above', 'pre-industrial', 'levels', '.', 'In', 'June', '2017', ',', 'U.S.', 'President', 'Donald', 'Trump', 'announced', 'his', 'intention', 'to', 'withdraw', 'the', 'United', 'States', 'from', 'the', 'agreement', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 1.173003147035209e-237\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), mid=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), high=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), mid=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), high=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333)), 'rougeLsum': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), mid=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), high=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The Paris Agreement is a big international agreement to tackle climate change. Countries are welcome to join any time, and they must commit to reducing their greenhouse gas emissions. All countries except the United States have pledged to cut carbon emissions between now and 2030. They are invited to increase their pledges every two years. Nations will also report on their projected levels of emissions every two years. Developed nations are expected to ramp up their commitments every two years starting in 2025. Developing nations are encouraged to consider increasing theirs around 2030. By the end of the agreement, countries should be committing to slash their greenhouse gas emissions by at least 50 percent below 2017 levels. This target represents a high-watermark for efforts, but scientists say achieving even more aggressive targets would further accelerate efforts to rein in global warming. The agreement was\n",
      "Tokenized Predictions: [['The', 'Paris', 'Agreement', 'is', 'a', 'big', 'international', 'agreement', 'to', 'tackle', 'climate', 'change', '.', 'Countries', 'are', 'welcome', 'to', 'join', 'any', 'time', ',', 'and', 'they', 'must', 'commit', 'to', 'reducing', 'their', 'greenhouse', 'gas', 'emissions', '.', 'All', 'countries', 'except', 'the', 'United', 'States', 'have', 'pledged', 'to', 'cut', 'carbon', 'emissions', 'between', 'now', 'and', '2030', '.', 'They', 'are', 'invited', 'to', 'increase', 'their', 'pledges', 'every', 'two', 'years', '.', 'Nations', 'will', 'also', 'report', 'on', 'their', 'projected', 'levels', 'of', 'emissions', 'every', 'two', 'years', '.', 'Developed', 'nations', 'are', 'expected', 'to', 'ramp', 'up', 'their', 'commitments', 'every', 'two', 'years', 'starting', 'in', '2025', '.', 'Developing', 'nations', 'are', 'encouraged', 'to', 'consider', 'increasing', 'theirs', 'around', '2030', '.', 'By', 'the', 'end', 'of', 'the', 'agreement', ',', 'countries', 'should', 'be', 'committing', 'to', 'slash', 'their', 'greenhouse', 'gas', 'emissions', 'by', 'at', 'least', '50', 'percent', 'below', '2017', 'levels', '.', 'This', 'target', 'represents', 'a', 'high-watermark', 'for', 'efforts', ',', 'but', 'scientists', 'say', 'achieving', 'even', 'more', 'aggressive', 'targets', 'would', 'further', 'accelerate', 'efforts', 'to', 'rein', 'in', 'global', 'warming', '.', 'The', 'agreement', 'was']]\n",
      "Tokenized References: [[['The', 'Paris', 'Agreement', 'is', 'a', 'big', 'international', 'agreement', 'to', 'tackle', 'climate', 'change', '.', '195', 'countries', 'signed', 'the', 'agreement', 'in', 'December', '2015', 'at', 'the', '21st', 'Conference', 'of', 'the', 'Parties', '(', 'COP21', ')', 'and', '186', 'have', 'become', 'party', 'to', 'it', '.', 'The', 'long-term', 'goal', 'is', 'to', 'keep', 'the', 'global', 'average', 'temperature', 'well', 'below', '2C', 'above', 'pre-industrial', 'levels', '.', 'In', 'June', '2017', ',', 'U.S.', 'President', 'Donald', 'Trump', 'announced', 'his', 'intention', 'to', 'withdraw', 'the', 'United', 'States', 'from', 'the', 'agreement', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.10955752810171694\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.23448275862068965, recall=0.4722222222222222, fmeasure=0.31336405529953915), mid=Score(precision=0.23448275862068965, recall=0.4722222222222222, fmeasure=0.31336405529953915), high=Score(precision=0.23448275862068965, recall=0.4722222222222222, fmeasure=0.31336405529953915)), 'rouge2': AggregateScore(low=Score(precision=0.1111111111111111, recall=0.22535211267605634, fmeasure=0.1488372093023256), mid=Score(precision=0.1111111111111111, recall=0.22535211267605634, fmeasure=0.1488372093023256), high=Score(precision=0.1111111111111111, recall=0.22535211267605634, fmeasure=0.1488372093023256)), 'rougeL': AggregateScore(low=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766), mid=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766), high=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766)), 'rougeLsum': AggregateScore(low=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766), mid=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766), high=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "So what happens at each COP?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Well, the first thing that happens is that the delegates gather in the plenary hall.\n",
      "Tokenized Predictions: [['Well', ',', 'the', 'first', 'thing', 'that', 'happens', 'is', 'that', 'the', 'delegates', 'gather', 'in', 'the', 'plenary', 'hall', '.']]\n",
      "Tokenized References: [[['The', 'United', 'Nations', 'Climate', 'Change', 'Conference', 'are', 'yearly', 'conferences', 'held', 'in', 'the', 'framework', 'of', 'the', 'UNFCCC', '.', 'They', 'serve', 'as', 'the', 'formal', 'meeting', 'of', 'the', 'UNFCCC', 'Parties', '(', 'Conferences', 'of', 'the', 'Parties', ')', '(', 'COP', ')', 'to', 'assess', 'progress', 'in', 'dealing', 'with', 'climate', 'change', 'and', 'beginning', 'in', 'the', 'mid-1990s', ',', 'to', 'negotiate', 'the', 'Kyoto', 'Protocol', 'to', 'establish', 'legally', 'binding', 'obligations', 'for', 'developed', 'countries', 'to', 'reduce', 'their', 'greenhouse', 'gas', 'emissions', '.', 'The', 'first', 'conference', '(', 'COP1', ')', 'was', 'held', 'in', '1995', 'in', 'Berlin', '.', 'Negotiations', 'for', 'the', 'Paris', 'Agreement', 'took', 'place', 'during', 'COP22', 'in', 'Marrakech', ',', 'Morocco', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 5.402139083364446e-157\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.3333333333333333, recall=0.05813953488372093, fmeasure=0.09900990099009901), mid=Score(precision=0.3333333333333333, recall=0.05813953488372093, fmeasure=0.09900990099009901), high=Score(precision=0.3333333333333333, recall=0.05813953488372093, fmeasure=0.09900990099009901)), 'rouge2': AggregateScore(low=Score(precision=0.14285714285714285, recall=0.023529411764705882, fmeasure=0.0404040404040404), mid=Score(precision=0.14285714285714285, recall=0.023529411764705882, fmeasure=0.0404040404040404), high=Score(precision=0.14285714285714285, recall=0.023529411764705882, fmeasure=0.0404040404040404)), 'rougeL': AggregateScore(low=Score(precision=0.26666666666666666, recall=0.046511627906976744, fmeasure=0.0792079207920792), mid=Score(precision=0.26666666666666666, recall=0.046511627906976744, fmeasure=0.0792079207920792), high=Score(precision=0.26666666666666666, recall=0.046511627906976744, fmeasure=0.0792079207920792)), 'rougeLsum': AggregateScore(low=Score(precision=0.26666666666666666, recall=0.046511627906976744, fmeasure=0.0792079207920792), mid=Score(precision=0.26666666666666666, recall=0.046511627906976744, fmeasure=0.0792079207920792), high=Score(precision=0.26666666666666666, recall=0.046511627906976744, fmeasure=0.0792079207920792))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The United Nations Climate Change Conference are yearly conferences held in the framework of the UNFCCC. They serve as a platform for communication and negotiation around global climate change, its expected impacts, and methods of tackling it. These conferences are known by the name of COP. There have been five such conferences since 1995 with twenty-two countries currently participating. Each conference outlines the intended levels of greenhouse gas emissions reductions undertaken voluntarily by various nations. Some developing countries are exempt from reporting on their targets due to their level of fossil fuel dependency. In February 2017, the Trump administration announced its intention to withdraw the United States from the agreement. Several other leaders have also expressed doubts about their country’s commitment towards emission cuts. Despite these setbacks, nearly all experts agree that the Paris Agreement remains intact for now. Why? Because countries who joined the accord will continue to report on their emissions until the U.S. officially\n",
      "Tokenized Predictions: [['The', 'United', 'Nations', 'Climate', 'Change', 'Conference', 'are', 'yearly', 'conferences', 'held', 'in', 'the', 'framework', 'of', 'the', 'UNFCCC', '.', 'They', 'serve', 'as', 'a', 'platform', 'for', 'communication', 'and', 'negotiation', 'around', 'global', 'climate', 'change', ',', 'its', 'expected', 'impacts', ',', 'and', 'methods', 'of', 'tackling', 'it', '.', 'These', 'conferences', 'are', 'known', 'by', 'the', 'name', 'of', 'COP', '.', 'There', 'have', 'been', 'five', 'such', 'conferences', 'since', '1995', 'with', 'twenty-two', 'countries', 'currently', 'participating', '.', 'Each', 'conference', 'outlines', 'the', 'intended', 'levels', 'of', 'greenhouse', 'gas', 'emissions', 'reductions', 'undertaken', 'voluntarily', 'by', 'various', 'nations', '.', 'Some', 'developing', 'countries', 'are', 'exempt', 'from', 'reporting', 'on', 'their', 'targets', 'due', 'to', 'their', 'level', 'of', 'fossil', 'fuel', 'dependency', '.', 'In', 'February', '2017', ',', 'the', 'Trump', 'administration', 'announced', 'its', 'intention', 'to', 'withdraw', 'the', 'United', 'States', 'from', 'the', 'agreement', '.', 'Several', 'other', 'leaders', 'have', 'also', 'expressed', 'doubts', 'about', 'their', 'country', '’', 's', 'commitment', 'towards', 'emission', 'cuts', '.', 'Despite', 'these', 'setbacks', ',', 'nearly', 'all', 'experts', 'agree', 'that', 'the', 'Paris', 'Agreement', 'remains', 'intact', 'for', 'now', '.', 'Why', '?', 'Because', 'countries', 'who', 'joined', 'the', 'accord', 'will', 'continue', 'to', 'report', 'on', 'their', 'emissions', 'until', 'the', 'U.S.', 'officially']]\n",
      "Tokenized References: [[['The', 'United', 'Nations', 'Climate', 'Change', 'Conference', 'are', 'yearly', 'conferences', 'held', 'in', 'the', 'framework', 'of', 'the', 'UNFCCC', '.', 'They', 'serve', 'as', 'the', 'formal', 'meeting', 'of', 'the', 'UNFCCC', 'Parties', '(', 'Conferences', 'of', 'the', 'Parties', ')', '(', 'COP', ')', 'to', 'assess', 'progress', 'in', 'dealing', 'with', 'climate', 'change', 'and', 'beginning', 'in', 'the', 'mid-1990s', ',', 'to', 'negotiate', 'the', 'Kyoto', 'Protocol', 'to', 'establish', 'legally', 'binding', 'obligations', 'for', 'developed', 'countries', 'to', 'reduce', 'their', 'greenhouse', 'gas', 'emissions', '.', 'The', 'first', 'conference', '(', 'COP1', ')', 'was', 'held', 'in', '1995', 'in', 'Berlin', '.', 'Negotiations', 'for', 'the', 'Paris', 'Agreement', 'took', 'place', 'during', 'COP22', 'in', 'Marrakech', ',', 'Morocco', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.1488224603768501\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.30625, recall=0.5697674418604651, fmeasure=0.39837398373983746), mid=Score(precision=0.30625, recall=0.5697674418604651, fmeasure=0.39837398373983746), high=Score(precision=0.30625, recall=0.5697674418604651, fmeasure=0.39837398373983746)), 'rouge2': AggregateScore(low=Score(precision=0.14465408805031446, recall=0.27058823529411763, fmeasure=0.1885245901639344), mid=Score(precision=0.14465408805031446, recall=0.27058823529411763, fmeasure=0.1885245901639344), high=Score(precision=0.14465408805031446, recall=0.27058823529411763, fmeasure=0.1885245901639344)), 'rougeL': AggregateScore(low=Score(precision=0.2125, recall=0.3953488372093023, fmeasure=0.2764227642276423), mid=Score(precision=0.2125, recall=0.3953488372093023, fmeasure=0.2764227642276423), high=Score(precision=0.2125, recall=0.3953488372093023, fmeasure=0.2764227642276423)), 'rougeLsum': AggregateScore(low=Score(precision=0.2125, recall=0.3953488372093023, fmeasure=0.2764227642276423), mid=Score(precision=0.2125, recall=0.3953488372093023, fmeasure=0.2764227642276423), high=Score(precision=0.2125, recall=0.3953488372093023, fmeasure=0.2764227642276423))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "How are the two different?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "The first one is a question, the second one is a statement.\n",
      "Tokenized Predictions: [['The', 'first', 'one', 'is', 'a', 'question', ',', 'the', 'second', 'one', 'is', 'a', 'statement', '.']]\n",
      "Tokenized References: [[['While', 'the', 'Kyoto', 'Protocol', 'and', 'Paris', 'Agreement', 'both', 'set', 'out', 'to', 'address', 'climate', 'change', ',', 'there', 'are', 'some', 'key', 'differences', 'between', 'them', '.', 'The', 'Kyoto', 'Protocol', 'established', 'top-down', 'legally', 'binding', 'emissions', 'reduction', 'targets', 'for', 'developed', 'nations', '.', 'On', 'the', 'other', 'hand', ',', 'the', 'Paris', 'Agreement', 'requires', 'that', 'all', 'countries', 'do', 'their', 'part', 'to', 'slash', 'greenhouse', 'gas', 'emissions', '.', 'No', 'language', 'is', 'included', 'about', 'the', 'commitments', 'countries', 'should', 'make', ';', 'nations', 'can', 'set', 'their', 'own', 'emissions', 'targets', 'consistent', 'with', 'their', 'level', 'of', 'development', 'and', 'technological', 'advancement', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 3.5471731745269624e-157\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782), mid=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782), high=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782), mid=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782), high=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782)), 'rougeLsum': AggregateScore(low=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782), mid=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782), high=Score(precision=0.25, recall=0.0375, fmeasure=0.06521739130434782))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The major difference between them is the power consumption. ARM-based processors consume less power as compared to x86. Since these devices run on batteries that are very small in capacity, energy conservation is extremely important. Also since lithium-ion batteries have a limited lifespan (about 500 charge cycles), keeping the power consumption low helps extend their lifetime. Another advantage of low power consumption is that smaller battery packs can be used for an equivalent amount of battery capacity. This means miniaturization of mobile phones becomes easier and cheaper. X86-based computers need more sophisticated cooling systems to keep the components at a comfortable temperature for reliable operation. These extra components add to the overall size and weight of the device. ARM-based processors do not require elaborate cooling mechanisms and hence, they are much smaller in size and lighter in weight when compared with x86-based smartphones. Because of less power consumption, ARM-based processors can be made\n",
      "Tokenized Predictions: [['The', 'major', 'difference', 'between', 'them', 'is', 'the', 'power', 'consumption', '.', 'ARM-based', 'processors', 'consume', 'less', 'power', 'as', 'compared', 'to', 'x86', '.', 'Since', 'these', 'devices', 'run', 'on', 'batteries', 'that', 'are', 'very', 'small', 'in', 'capacity', ',', 'energy', 'conservation', 'is', 'extremely', 'important', '.', 'Also', 'since', 'lithium-ion', 'batteries', 'have', 'a', 'limited', 'lifespan', '(', 'about', '500', 'charge', 'cycles', ')', ',', 'keeping', 'the', 'power', 'consumption', 'low', 'helps', 'extend', 'their', 'lifetime', '.', 'Another', 'advantage', 'of', 'low', 'power', 'consumption', 'is', 'that', 'smaller', 'battery', 'packs', 'can', 'be', 'used', 'for', 'an', 'equivalent', 'amount', 'of', 'battery', 'capacity', '.', 'This', 'means', 'miniaturization', 'of', 'mobile', 'phones', 'becomes', 'easier', 'and', 'cheaper', '.', 'X86-based', 'computers', 'need', 'more', 'sophisticated', 'cooling', 'systems', 'to', 'keep', 'the', 'components', 'at', 'a', 'comfortable', 'temperature', 'for', 'reliable', 'operation', '.', 'These', 'extra', 'components', 'add', 'to', 'the', 'overall', 'size', 'and', 'weight', 'of', 'the', 'device', '.', 'ARM-based', 'processors', 'do', 'not', 'require', 'elaborate', 'cooling', 'mechanisms', 'and', 'hence', ',', 'they', 'are', 'much', 'smaller', 'in', 'size', 'and', 'lighter', 'in', 'weight', 'when', 'compared', 'with', 'x86-based', 'smartphones', '.', 'Because', 'of', 'less', 'power', 'consumption', ',', 'ARM-based', 'processors', 'can', 'be', 'made']]\n",
      "Tokenized References: [[['While', 'the', 'Kyoto', 'Protocol', 'and', 'Paris', 'Agreement', 'both', 'set', 'out', 'to', 'address', 'climate', 'change', ',', 'there', 'are', 'some', 'key', 'differences', 'between', 'them', '.', 'The', 'Kyoto', 'Protocol', 'established', 'top-down', 'legally', 'binding', 'emissions', 'reduction', 'targets', 'for', 'developed', 'nations', '.', 'On', 'the', 'other', 'hand', ',', 'the', 'Paris', 'Agreement', 'requires', 'that', 'all', 'countries', 'do', 'their', 'part', 'to', 'slash', 'greenhouse', 'gas', 'emissions', '.', 'No', 'language', 'is', 'included', 'about', 'the', 'commitments', 'countries', 'should', 'make', ';', 'nations', 'can', 'set', 'their', 'own', 'emissions', 'targets', 'consistent', 'with', 'their', 'level', 'of', 'development', 'and', 'technological', 'advancement', '.']]]\n",
      "PEFT MODEL BLEU Score: 2.6272821701574408e-155\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.13836477987421383, recall=0.275, fmeasure=0.18410041841004188), mid=Score(precision=0.13836477987421383, recall=0.275, fmeasure=0.18410041841004188), high=Score(precision=0.13836477987421383, recall=0.275, fmeasure=0.18410041841004188)), 'rouge2': AggregateScore(low=Score(precision=0.006329113924050633, recall=0.012658227848101266, fmeasure=0.008438818565400843), mid=Score(precision=0.006329113924050633, recall=0.012658227848101266, fmeasure=0.008438818565400843), high=Score(precision=0.006329113924050633, recall=0.012658227848101266, fmeasure=0.008438818565400843)), 'rougeL': AggregateScore(low=Score(precision=0.07547169811320754, recall=0.15, fmeasure=0.10041841004184099), mid=Score(precision=0.07547169811320754, recall=0.15, fmeasure=0.10041841004184099), high=Score(precision=0.07547169811320754, recall=0.15, fmeasure=0.10041841004184099)), 'rougeLsum': AggregateScore(low=Score(precision=0.07547169811320754, recall=0.15, fmeasure=0.10041841004184099), mid=Score(precision=0.07547169811320754, recall=0.15, fmeasure=0.10041841004184099), high=Score(precision=0.07547169811320754, recall=0.15, fmeasure=0.10041841004184099))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "I remember Glasgow hosting COP26 last year, but unfortunately I was out of the loop. What was it about?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It was a conference on climate change.\n",
      "Tokenized Predictions: [['It', 'was', 'a', 'conference', 'on', 'climate', 'change', '.']]\n",
      "Tokenized References: [[['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'The', 'term', '“', 'climate', 'change', '”', 'is', 'often', 'used', 'as', 'if', 'it', 'means', 'the', 'same', 'thing', 'as', 'the', 'term', '“', 'global', 'warming', '”', '.', 'The', 'National', 'Academy', 'of', 'Sciences', 'says', '“', 'climate', 'change', '”', 'is', 'growing', 'in', 'favor', 'of', '“', 'global', 'warming', '”', 'because', 'it', 'helps', 'convey', 'that', 'there', 'are', 'other', 'changes', 'in', 'addition', 'to', 'rising', 'temperatures', '.', 'In', 'fact', ',', '“', 'climate', 'change', '”', 'means', 'major', 'changes', 'in', 'temperature', ',', 'rainfall', ',', 'snow', ',', 'or', 'wind', 'patterns', 'lasting', 'for', 'decades', 'or', 'longer', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 4.181379769687496e-160\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342), mid=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342), high=Score(precision=0.5714285714285714, recall=0.04597701149425287, fmeasure=0.08510638297872342)), 'rouge2': AggregateScore(low=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261), mid=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261), high=Score(precision=0.16666666666666666, recall=0.011627906976744186, fmeasure=0.02173913043478261)), 'rougeL': AggregateScore(low=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), mid=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), high=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255)), 'rougeLsum': AggregateScore(low=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), mid=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255), high=Score(precision=0.42857142857142855, recall=0.034482758620689655, fmeasure=0.06382978723404255))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The COP26 event is a global united Nations summit about climate change and how countries are planning to tackle it. Name a conference that doesn't have to say something important every single day? No, they aren't all equal. Climate change is a big problem. It affects all members of the human race and any living thing on our wonderful planet. Since 1985, the United Nations has gathered at least once annually for the UNFCCC COP. There have been 26 conferences total. The biggest issue: money. Developing nations are only recently beginning to talk about carbon neutrality around the turn of the century. For many years, the U.S. was one of the largest roadblocks in the way of international cooperation to fight climate change. President Donald Trump withdrew from the Paris Agreement in 2017, making the U.S\n",
      "Tokenized Predictions: [['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'Name', 'a', 'conference', 'that', 'does', \"n't\", 'have', 'to', 'say', 'something', 'important', 'every', 'single', 'day', '?', 'No', ',', 'they', 'are', \"n't\", 'all', 'equal', '.', 'Climate', 'change', 'is', 'a', 'big', 'problem', '.', 'It', 'affects', 'all', 'members', 'of', 'the', 'human', 'race', 'and', 'any', 'living', 'thing', 'on', 'our', 'wonderful', 'planet', '.', 'Since', '1985', ',', 'the', 'United', 'Nations', 'has', 'gathered', 'at', 'least', 'once', 'annually', 'for', 'the', 'UNFCCC', 'COP', '.', 'There', 'have', 'been', '26', 'conferences', 'total', '.', 'The', 'biggest', 'issue', ':', 'money', '.', 'Developing', 'nations', 'are', 'only', 'recently', 'beginning', 'to', 'talk', 'about', 'carbon', 'neutrality', 'around', 'the', 'turn', 'of', 'the', 'century', '.', 'For', 'many', 'years', ',', 'the', 'U.S.', 'was', 'one', 'of', 'the', 'largest', 'roadblocks', 'in', 'the', 'way', 'of', 'international', 'cooperation', 'to', 'fight', 'climate', 'change', '.', 'President', 'Donald', 'Trump', 'withdrew', 'from', 'the', 'Paris', 'Agreement', 'in', '2017', ',', 'making', 'the', 'U.S']]\n",
      "Tokenized References: [[['The', 'COP26', 'event', 'is', 'a', 'global', 'united', 'Nations', 'summit', 'about', 'climate', 'change', 'and', 'how', 'countries', 'are', 'planning', 'to', 'tackle', 'it', '.', 'The', 'term', '“', 'climate', 'change', '”', 'is', 'often', 'used', 'as', 'if', 'it', 'means', 'the', 'same', 'thing', 'as', 'the', 'term', '“', 'global', 'warming', '”', '.', 'The', 'National', 'Academy', 'of', 'Sciences', 'says', '“', 'climate', 'change', '”', 'is', 'growing', 'in', 'favor', 'of', '“', 'global', 'warming', '”', 'because', 'it', 'helps', 'convey', 'that', 'there', 'are', 'other', 'changes', 'in', 'addition', 'to', 'rising', 'temperatures', '.', 'In', 'fact', ',', '“', 'climate', 'change', '”', 'means', 'major', 'changes', 'in', 'temperature', ',', 'rainfall', ',', 'snow', ',', 'or', 'wind', 'patterns', 'lasting', 'for', 'decades', 'or', 'longer', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.15833413710075372\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675), mid=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675), high=Score(precision=0.2857142857142857, recall=0.45977011494252873, fmeasure=0.35242290748898675)), 'rouge2': AggregateScore(low=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557), mid=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557), high=Score(precision=0.15827338129496402, recall=0.2558139534883721, fmeasure=0.19555555555555557)), 'rougeL': AggregateScore(low=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), mid=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), high=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634)), 'rougeLsum': AggregateScore(low=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), mid=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634), high=Score(precision=0.25, recall=0.40229885057471265, fmeasure=0.3083700440528634))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Interesting. What are the effects of these changes?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "The effects are as follows:\n",
      "Tokenized Predictions: [['The', 'effects', 'are', 'as', 'follows', ':']]\n",
      "Tokenized References: [[['Climate', 'change', 'is', 'very', 'likely', 'having', 'an', 'impact', 'now', 'on', 'our', 'planet', 'and', 'its', 'life', ',', 'according', 'to', 'the', 'latest', 'instalment', 'of', 'a', 'report', 'published', 'by', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', '(', 'IPCC', ')', '.', 'And', 'the', 'future', 'problems', 'caused', 'by', 'rising', 'seas', ',', 'growing', 'deserts', ',', 'and', 'more', 'frequent', 'droughts', 'all', 'look', 'set', 'to', 'affect', 'the', 'developing', 'world', 'more', 'than', 'rich', 'countries', ',', 'they', 'add', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 0\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077)), 'rougeLsum': AggregateScore(low=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), mid=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077), high=Score(precision=0.2, recall=0.016666666666666666, fmeasure=0.03076923076923077))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Climate change is a huge problem that affects the planet and all its life. Here on Earth, it causes problems such as rising sea levels, changing weather patterns and the extinction of species. Worse still, climate change also caused the Antarctic ice sheet to collapse, which made the ocean water more salty and acidic. In addition, it affected the Gulf Stream, which was transporting warm waters from Mexico into Europe. This slowed down or stopped completely resulting in cold temperatures across much of the continent. The future looks even bleaker as the melting Arctic ice will lead to longer and deeper temperature dips. As the ice melts, dark Arctic waters will absorb more heat, causing further ice melt. A never-ending cycle unless we find a way to break it. The world’s leading scientists now agree that climate change is not only a scientific fact, but it is having – and will continue to have – a serious impact\n",
      "Tokenized Predictions: [['Climate', 'change', 'is', 'a', 'huge', 'problem', 'that', 'affects', 'the', 'planet', 'and', 'all', 'its', 'life', '.', 'Here', 'on', 'Earth', ',', 'it', 'causes', 'problems', 'such', 'as', 'rising', 'sea', 'levels', ',', 'changing', 'weather', 'patterns', 'and', 'the', 'extinction', 'of', 'species', '.', 'Worse', 'still', ',', 'climate', 'change', 'also', 'caused', 'the', 'Antarctic', 'ice', 'sheet', 'to', 'collapse', ',', 'which', 'made', 'the', 'ocean', 'water', 'more', 'salty', 'and', 'acidic', '.', 'In', 'addition', ',', 'it', 'affected', 'the', 'Gulf', 'Stream', ',', 'which', 'was', 'transporting', 'warm', 'waters', 'from', 'Mexico', 'into', 'Europe', '.', 'This', 'slowed', 'down', 'or', 'stopped', 'completely', 'resulting', 'in', 'cold', 'temperatures', 'across', 'much', 'of', 'the', 'continent', '.', 'The', 'future', 'looks', 'even', 'bleaker', 'as', 'the', 'melting', 'Arctic', 'ice', 'will', 'lead', 'to', 'longer', 'and', 'deeper', 'temperature', 'dips', '.', 'As', 'the', 'ice', 'melts', ',', 'dark', 'Arctic', 'waters', 'will', 'absorb', 'more', 'heat', ',', 'causing', 'further', 'ice', 'melt', '.', 'A', 'never-ending', 'cycle', 'unless', 'we', 'find', 'a', 'way', 'to', 'break', 'it', '.', 'The', 'world', '’', 's', 'leading', 'scientists', 'now', 'agree', 'that', 'climate', 'change', 'is', 'not', 'only', 'a', 'scientific', 'fact', ',', 'but', 'it', 'is', 'having', '–', 'and', 'will', 'continue', 'to', 'have', '–', 'a', 'serious', 'impact']]\n",
      "Tokenized References: [[['Climate', 'change', 'is', 'very', 'likely', 'having', 'an', 'impact', 'now', 'on', 'our', 'planet', 'and', 'its', 'life', ',', 'according', 'to', 'the', 'latest', 'instalment', 'of', 'a', 'report', 'published', 'by', 'the', 'Intergovernmental', 'Panel', 'on', 'Climate', 'Change', '(', 'IPCC', ')', '.', 'And', 'the', 'future', 'problems', 'caused', 'by', 'rising', 'seas', ',', 'growing', 'deserts', ',', 'and', 'more', 'frequent', 'droughts', 'all', 'look', 'set', 'to', 'affect', 'the', 'developing', 'world', 'more', 'than', 'rich', 'countries', ',', 'they', 'add', '.']]]\n",
      "PEFT MODEL BLEU Score: 8.631570841076429e-79\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706), mid=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706), high=Score(precision=0.1962025316455696, recall=0.5166666666666667, fmeasure=0.2844036697247706)), 'rouge2': AggregateScore(low=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481), mid=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481), high=Score(precision=0.044585987261146494, recall=0.11864406779661017, fmeasure=0.06481481481481481)), 'rougeL': AggregateScore(low=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), mid=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), high=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046)), 'rougeLsum': AggregateScore(low=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), mid=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046), high=Score(precision=0.12025316455696203, recall=0.31666666666666665, fmeasure=0.1743119266055046))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s interesting. Tell me more.\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "I’m afraid I can’t.\n",
      "Tokenized Predictions: [['I', '’', 'm', 'afraid', 'I', 'can', '’', 't', '.']]\n",
      "Tokenized References: [[['For', 'several', 'years', ',', 'there', 'have', 'been', 'concerns', 'that', 'climate', 'change', 'negotiations', 'will', 'essentially', 'ignore', 'a', 'key', 'principle', 'of', 'climate', 'change', 'negotiation', 'frameworks', ':', 'the', 'common', 'but', 'differentiated', 'responsibilities', '.', 'Realizing', 'that', 'greenhouse', 'emissions', 'remain', 'in', 'the', 'atmosphere', 'for', 'a', 'very', 'long', 'time', ',', 'this', 'principle', 'recognizes', 'that', 'historically', ':', 'Industrialized', 'nations', 'have', 'emitted', 'far', 'more', 'greenhouse', 'gas', 'emissions', '(', 'even', 'if', 'some', 'developing', 'nations', 'are', 'only', 'now', 'increasing', 'theirs', ')', ';', 'Rich', 'countries', ',', 'therefore', ',', 'face', 'the', 'biggest', 'responsibility', 'and', 'burden', 'for', 'action', 'to', 'address', 'climate', 'change', ';', 'and', 'Rich', 'countries', ',', 'therefore', ',', 'must', 'support', 'developing', 'nations', 'adapt—through', 'financing', 'and', 'technology', 'transfer', ',', 'for', 'example', '.', 'This', 'notion', 'of', 'climate', 'justice', 'is', 'typically', 'ignored', 'by', 'many', 'rich', 'nations', 'and', 'their', 'mainstream', 'media', ',', 'making', 'it', 'easy', 'to', 'blame', 'China', ',', 'India', 'and', 'other', 'developing', 'countries', 'for', 'failures', 'in', 'climate', 'change', 'mitigation', 'negotiations', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 2.576443261051062e-238\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeLsum': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "A lot of online activities are tracked in some way; most visitors can be followed in at least two ways: by placing a tracking cookie, which is essentially a unique identifier, and by logging information voluntarily (e.g., through a login), as well as other means such as camera recognition, fingerprinting, and many more. Websites track users for various reasons, mostly for advertising, data mining, and targeting ads. The latter includes both promoting products and manipulating people. Tracking prevents ad blockers from blocking all advertisements. At the same time, advertisers have the option to be excluded from tracking. There are also alternative methods of blocking some trackers but not others. Adblock Plus blocks certain types of trackers but not webfonts, for example. DoEzApps browser extension blocks almost all trackers but allows script and style files to load. Ghostery is another popular ad blocker that does not block all trackers but instead\n",
      "Tokenized Predictions: [['A', 'lot', 'of', 'online', 'activities', 'are', 'tracked', 'in', 'some', 'way', ';', 'most', 'visitors', 'can', 'be', 'followed', 'in', 'at', 'least', 'two', 'ways', ':', 'by', 'placing', 'a', 'tracking', 'cookie', ',', 'which', 'is', 'essentially', 'a', 'unique', 'identifier', ',', 'and', 'by', 'logging', 'information', 'voluntarily', '(', 'e.g.', ',', 'through', 'a', 'login', ')', ',', 'as', 'well', 'as', 'other', 'means', 'such', 'as', 'camera', 'recognition', ',', 'fingerprinting', ',', 'and', 'many', 'more', '.', 'Websites', 'track', 'users', 'for', 'various', 'reasons', ',', 'mostly', 'for', 'advertising', ',', 'data', 'mining', ',', 'and', 'targeting', 'ads', '.', 'The', 'latter', 'includes', 'both', 'promoting', 'products', 'and', 'manipulating', 'people', '.', 'Tracking', 'prevents', 'ad', 'blockers', 'from', 'blocking', 'all', 'advertisements', '.', 'At', 'the', 'same', 'time', ',', 'advertisers', 'have', 'the', 'option', 'to', 'be', 'excluded', 'from', 'tracking', '.', 'There', 'are', 'also', 'alternative', 'methods', 'of', 'blocking', 'some', 'trackers', 'but', 'not', 'others', '.', 'Adblock', 'Plus', 'blocks', 'certain', 'types', 'of', 'trackers', 'but', 'not', 'webfonts', ',', 'for', 'example', '.', 'DoEzApps', 'browser', 'extension', 'blocks', 'almost', 'all', 'trackers', 'but', 'allows', 'script', 'and', 'style', 'files', 'to', 'load', '.', 'Ghostery', 'is', 'another', 'popular', 'ad', 'blocker', 'that', 'does', 'not', 'block', 'all', 'trackers', 'but', 'instead']]\n",
      "Tokenized References: [[['For', 'several', 'years', ',', 'there', 'have', 'been', 'concerns', 'that', 'climate', 'change', 'negotiations', 'will', 'essentially', 'ignore', 'a', 'key', 'principle', 'of', 'climate', 'change', 'negotiation', 'frameworks', ':', 'the', 'common', 'but', 'differentiated', 'responsibilities', '.', 'Realizing', 'that', 'greenhouse', 'emissions', 'remain', 'in', 'the', 'atmosphere', 'for', 'a', 'very', 'long', 'time', ',', 'this', 'principle', 'recognizes', 'that', 'historically', ':', 'Industrialized', 'nations', 'have', 'emitted', 'far', 'more', 'greenhouse', 'gas', 'emissions', '(', 'even', 'if', 'some', 'developing', 'nations', 'are', 'only', 'now', 'increasing', 'theirs', ')', ';', 'Rich', 'countries', ',', 'therefore', ',', 'face', 'the', 'biggest', 'responsibility', 'and', 'burden', 'for', 'action', 'to', 'address', 'climate', 'change', ';', 'and', 'Rich', 'countries', ',', 'therefore', ',', 'must', 'support', 'developing', 'nations', 'adapt—through', 'financing', 'and', 'technology', 'transfer', ',', 'for', 'example', '.', 'This', 'notion', 'of', 'climate', 'justice', 'is', 'typically', 'ignored', 'by', 'many', 'rich', 'nations', 'and', 'their', 'mainstream', 'media', ',', 'making', 'it', 'easy', 'to', 'blame', 'China', ',', 'India', 'and', 'other', 'developing', 'countries', 'for', 'failures', 'in', 'climate', 'change', 'mitigation', 'negotiations', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.025676869023137934\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.2251655629139073, recall=0.26356589147286824, fmeasure=0.24285714285714285), mid=Score(precision=0.2251655629139073, recall=0.26356589147286824, fmeasure=0.24285714285714285), high=Score(precision=0.2251655629139073, recall=0.26356589147286824, fmeasure=0.24285714285714285)), 'rouge2': AggregateScore(low=Score(precision=0.006666666666666667, recall=0.0078125, fmeasure=0.007194244604316546), mid=Score(precision=0.006666666666666667, recall=0.0078125, fmeasure=0.007194244604316546), high=Score(precision=0.006666666666666667, recall=0.0078125, fmeasure=0.007194244604316546)), 'rougeL': AggregateScore(low=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572), mid=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572), high=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572)), 'rougeLsum': AggregateScore(low=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572), mid=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572), high=Score(precision=0.07947019867549669, recall=0.09302325581395349, fmeasure=0.08571428571428572))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Okay, but how does it affect developing countries?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It doesn't.\n",
      "Tokenized Predictions: [['It', 'does', \"n't\", '.']]\n",
      "Tokenized References: [[['Developing', 'countries', 'are', 'the', 'least', 'able', 'to', 'adapt', 'to', 'climate', 'change', '(', 'and', 'therefore', 'called', '``', 'highly', 'climate', 'vulnerable', \"''\", ')', 'due', 'to', 'their', 'low', 'levels', 'of', 'wealth', ',', 'technology', ',', 'education', ',', 'and', 'access', 'to', 'resources', '.', 'Climate', 'change', 'causes', '400,000', 'deaths', 'on', 'average', 'each', 'year', ',', 'mainly', 'due', 'to', 'hunger', 'and', 'diseases', 'in', 'developing', 'countries', '.', 'Interestingly', ',', 'many', 'people', 'in', 'developing', 'nations', 'who', 'had', 'never', 'been', 'educated', 'about', 'climate', 'change', 'have', 'noticed', 'local', 'changes', 'in', 'temperature', 'and', 'precipitation', 'patterns', 'according', 'to', 'a', 'study', 'published', 'in', 'the', 'journal', 'Nature', 'Climate', 'Change', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 2.1795532445072597e-241\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeLsum': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Developing countries are the least able to adapt to climate change (and therefore called \"highly climate vulnerable\") due to their low levels of wealth, technology, education, and access to resources. Climate change causes 400,000 deaths on average each year, mainly in developing nations, largely due to hunger and diseases in impoverished communities. As the temperature continues to rise, rising seas will swallow more and more of the land, forcing population shifts in the days of increased coastal migration. At the same time, ice that once protects against these changes will melt, leading to longer periods of global freezing. The waves will get worse; every one years severe weather events caused by stalled fronts or cyclones will reach the Earth’s coldest surfaces. Furthermore, as the polar caps melt during the winter season, less sunlight will reach the earth causing colder temperatures for a longer period. In addition, the frozen sea will\n",
      "Tokenized Predictions: [['Developing', 'countries', 'are', 'the', 'least', 'able', 'to', 'adapt', 'to', 'climate', 'change', '(', 'and', 'therefore', 'called', '``', 'highly', 'climate', 'vulnerable', \"''\", ')', 'due', 'to', 'their', 'low', 'levels', 'of', 'wealth', ',', 'technology', ',', 'education', ',', 'and', 'access', 'to', 'resources', '.', 'Climate', 'change', 'causes', '400,000', 'deaths', 'on', 'average', 'each', 'year', ',', 'mainly', 'in', 'developing', 'nations', ',', 'largely', 'due', 'to', 'hunger', 'and', 'diseases', 'in', 'impoverished', 'communities', '.', 'As', 'the', 'temperature', 'continues', 'to', 'rise', ',', 'rising', 'seas', 'will', 'swallow', 'more', 'and', 'more', 'of', 'the', 'land', ',', 'forcing', 'population', 'shifts', 'in', 'the', 'days', 'of', 'increased', 'coastal', 'migration', '.', 'At', 'the', 'same', 'time', ',', 'ice', 'that', 'once', 'protects', 'against', 'these', 'changes', 'will', 'melt', ',', 'leading', 'to', 'longer', 'periods', 'of', 'global', 'freezing', '.', 'The', 'waves', 'will', 'get', 'worse', ';', 'every', 'one', 'years', 'severe', 'weather', 'events', 'caused', 'by', 'stalled', 'fronts', 'or', 'cyclones', 'will', 'reach', 'the', 'Earth', '’', 's', 'coldest', 'surfaces', '.', 'Furthermore', ',', 'as', 'the', 'polar', 'caps', 'melt', 'during', 'the', 'winter', 'season', ',', 'less', 'sunlight', 'will', 'reach', 'the', 'earth', 'causing', 'colder', 'temperatures', 'for', 'a', 'longer', 'period', '.', 'In', 'addition', ',', 'the', 'frozen', 'sea', 'will']]\n",
      "Tokenized References: [[['Developing', 'countries', 'are', 'the', 'least', 'able', 'to', 'adapt', 'to', 'climate', 'change', '(', 'and', 'therefore', 'called', '``', 'highly', 'climate', 'vulnerable', \"''\", ')', 'due', 'to', 'their', 'low', 'levels', 'of', 'wealth', ',', 'technology', ',', 'education', ',', 'and', 'access', 'to', 'resources', '.', 'Climate', 'change', 'causes', '400,000', 'deaths', 'on', 'average', 'each', 'year', ',', 'mainly', 'due', 'to', 'hunger', 'and', 'diseases', 'in', 'developing', 'countries', '.', 'Interestingly', ',', 'many', 'people', 'in', 'developing', 'nations', 'who', 'had', 'never', 'been', 'educated', 'about', 'climate', 'change', 'have', 'noticed', 'local', 'changes', 'in', 'temperature', 'and', 'precipitation', 'patterns', 'according', 'to', 'a', 'study', 'published', 'in', 'the', 'journal', 'Nature', 'Climate', 'Change', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.32168708779236976\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.3815789473684211, recall=0.6987951807228916, fmeasure=0.4936170212765958), mid=Score(precision=0.3815789473684211, recall=0.6987951807228916, fmeasure=0.4936170212765958), high=Score(precision=0.3815789473684211, recall=0.6987951807228916, fmeasure=0.4936170212765958)), 'rouge2': AggregateScore(low=Score(precision=0.31788079470198677, recall=0.5853658536585366, fmeasure=0.4120171673819743), mid=Score(precision=0.31788079470198677, recall=0.5853658536585366, fmeasure=0.4120171673819743), high=Score(precision=0.31788079470198677, recall=0.5853658536585366, fmeasure=0.4120171673819743)), 'rougeL': AggregateScore(low=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234), mid=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234), high=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234)), 'rougeLsum': AggregateScore(low=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234), mid=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234), high=Score(precision=0.34868421052631576, recall=0.6385542168674698, fmeasure=0.451063829787234))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "How are developed countries helping with that?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "They are helping by providing financial aid to developing countries.\n",
      "Tokenized Predictions: [['They', 'are', 'helping', 'by', 'providing', 'financial', 'aid', 'to', 'developing', 'countries', '.']]\n",
      "Tokenized References: [[['Well', ',', 'according', 'to', 'the', 'Paris', 'Agreement', ',', 'every', 'two', 'years', ',', 'developed', 'countries', 'are', 'to', 'communicate', 'the', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'they', 'provide', 'to', 'developing', 'countries', 'to', 'mitigate', 'and', 'adapt', 'to', 'climate', 'change', '.', 'Some', 'developing', 'countries', 'also', 'contribute', 'to', 'climate', 'finance', 'too', '.', 'Already', ',', 'Brazil', '’', 's', 'President', 'Dilma', 'Rousseff', 'said', 'the', 'country', 'is', 'considering', 'contributing', 'to', 'climate', 'finance', ',', 'joining', 'other', 'emerging', 'economies', 'like', 'China', ',', 'which', 'pledged', 'to', 'provide', '$', '3.1', 'billion', 'over', 'three', 'years', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 5.56253195093122e-81\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.5, recall=0.0684931506849315, fmeasure=0.12048192771084336), mid=Score(precision=0.5, recall=0.0684931506849315, fmeasure=0.12048192771084336), high=Score(precision=0.5, recall=0.0684931506849315, fmeasure=0.12048192771084336)), 'rouge2': AggregateScore(low=Score(precision=0.2222222222222222, recall=0.027777777777777776, fmeasure=0.04938271604938271), mid=Score(precision=0.2222222222222222, recall=0.027777777777777776, fmeasure=0.04938271604938271), high=Score(precision=0.2222222222222222, recall=0.027777777777777776, fmeasure=0.04938271604938271)), 'rougeL': AggregateScore(low=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469), mid=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469), high=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469)), 'rougeLsum': AggregateScore(low=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469), mid=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469), high=Score(precision=0.4, recall=0.0547945205479452, fmeasure=0.09638554216867469))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Well, according to the Paris Agreement, every two years, developed countries are expected to communicate the projected levels of public climate finance they provide to developing countries to mitigate and adapt to climate change. Some developing countries also contribute to climate finance too. Already, Brazil’s President Dilma Rousseff said the country is considering contributing to climate finance, joining other emerging economies like China, which pledged to provide $3.1 billion over three years. The biggest provider historically has been the United States, despite Republican opposition. Other major contributors include European nations, Japan, and the World Bank Group. Climate finance is intended to help developing countries adapt to rising seas, growing deserts caused by global warming, and money to get rid of their carbon pollution. Rich countries will provide all climate finance necessary to develop emerging markets and developing countries – but they do not say what it will be for. Already, the Trump administration is trying\n",
      "Tokenized Predictions: [['Well', ',', 'according', 'to', 'the', 'Paris', 'Agreement', ',', 'every', 'two', 'years', ',', 'developed', 'countries', 'are', 'expected', 'to', 'communicate', 'the', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'they', 'provide', 'to', 'developing', 'countries', 'to', 'mitigate', 'and', 'adapt', 'to', 'climate', 'change', '.', 'Some', 'developing', 'countries', 'also', 'contribute', 'to', 'climate', 'finance', 'too', '.', 'Already', ',', 'Brazil', '’', 's', 'President', 'Dilma', 'Rousseff', 'said', 'the', 'country', 'is', 'considering', 'contributing', 'to', 'climate', 'finance', ',', 'joining', 'other', 'emerging', 'economies', 'like', 'China', ',', 'which', 'pledged', 'to', 'provide', '$', '3.1', 'billion', 'over', 'three', 'years', '.', 'The', 'biggest', 'provider', 'historically', 'has', 'been', 'the', 'United', 'States', ',', 'despite', 'Republican', 'opposition', '.', 'Other', 'major', 'contributors', 'include', 'European', 'nations', ',', 'Japan', ',', 'and', 'the', 'World', 'Bank', 'Group', '.', 'Climate', 'finance', 'is', 'intended', 'to', 'help', 'developing', 'countries', 'adapt', 'to', 'rising', 'seas', ',', 'growing', 'deserts', 'caused', 'by', 'global', 'warming', ',', 'and', 'money', 'to', 'get', 'rid', 'of', 'their', 'carbon', 'pollution', '.', 'Rich', 'countries', 'will', 'provide', 'all', 'climate', 'finance', 'necessary', 'to', 'develop', 'emerging', 'markets', 'and', 'developing', 'countries', '–', 'but', 'they', 'do', 'not', 'say', 'what', 'it', 'will', 'be', 'for', '.', 'Already', ',', 'the', 'Trump', 'administration', 'is', 'trying']]\n",
      "Tokenized References: [[['Well', ',', 'according', 'to', 'the', 'Paris', 'Agreement', ',', 'every', 'two', 'years', ',', 'developed', 'countries', 'are', 'to', 'communicate', 'the', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'they', 'provide', 'to', 'developing', 'countries', 'to', 'mitigate', 'and', 'adapt', 'to', 'climate', 'change', '.', 'Some', 'developing', 'countries', 'also', 'contribute', 'to', 'climate', 'finance', 'too', '.', 'Already', ',', 'Brazil', '’', 's', 'President', 'Dilma', 'Rousseff', 'said', 'the', 'country', 'is', 'considering', 'contributing', 'to', 'climate', 'finance', ',', 'joining', 'other', 'emerging', 'economies', 'like', 'China', ',', 'which', 'pledged', 'to', 'provide', '$', '3.1', 'billion', 'over', 'three', 'years', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.4556715605238871\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), mid=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), high=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384)), 'rouge2': AggregateScore(low=Score(precision=0.45806451612903226, recall=0.9861111111111112, fmeasure=0.6255506607929516), mid=Score(precision=0.45806451612903226, recall=0.9861111111111112, fmeasure=0.6255506607929516), high=Score(precision=0.45806451612903226, recall=0.9861111111111112, fmeasure=0.6255506607929516)), 'rougeL': AggregateScore(low=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), mid=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), high=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384)), 'rougeLsum': AggregateScore(low=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), mid=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384), high=Score(precision=0.46794871794871795, recall=1.0, fmeasure=0.6375545851528384))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Are they meeting them?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Yes, they are.\n",
      "Tokenized Predictions: [['Yes', ',', 'they', 'are', '.']]\n",
      "Tokenized References: [[['Developed', 'countries', 'will', 'communicate', 'every', 'two', 'years', 'on', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'to', 'be', 'provided', 'to', 'developing', 'countries', '.', 'Developing', 'countries', 'will', 'report', 'on', 'climate', 'finance', 'they', 'will', 'provide', 'voluntarily', '.', 'Developed', 'countries', 'are', 'not', 'obligated', 'to', 'provide', 'climate', 'finance', ',', 'but', 'they', 'are', 'welcome', 'to', 'contribute', 'financing', 'for', 'climate', 'activities', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 5.531511147753189e-159\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), mid=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), high=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754)), 'rouge2': AggregateScore(low=Score(precision=0.5, recall=0.02040816326530612, fmeasure=0.039215686274509796), mid=Score(precision=0.5, recall=0.02040816326530612, fmeasure=0.039215686274509796), high=Score(precision=0.5, recall=0.02040816326530612, fmeasure=0.039215686274509796)), 'rougeL': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), mid=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), high=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754)), 'rougeLsum': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), mid=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754), high=Score(precision=0.6666666666666666, recall=0.04, fmeasure=0.07547169811320754))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The International Space Station (ISS) is one of two stations which have been or currently are hosted by NASA. The station since its inception has provided a long-term human presence in space and serves as a laboratory for scientific research, demonstration, and technology development. It consists of components that together form an environment where no terrestrial forces are needed for crew members to carry out their tasks. With over 13 trillion cubic metres of air being supplied for breathing and insulation, it surpasses Earth's surface and is by far the largest human-made structure in space. Due to the magnitude of work conducted by the ISS, disagreements arose between nations regarding distribution of financial contributions. There have also been debates about which country she represents the greatest achievement for. This paper seeks to outline the reasons why the National Academy of Sciences concluded that Hall’s statement was false and caused substantial harm. \n",
      "[Asking how it causes harm] How did his\n",
      "Tokenized Predictions: [['The', 'International', 'Space', 'Station', '(', 'ISS', ')', 'is', 'one', 'of', 'two', 'stations', 'which', 'have', 'been', 'or', 'currently', 'are', 'hosted', 'by', 'NASA', '.', 'The', 'station', 'since', 'its', 'inception', 'has', 'provided', 'a', 'long-term', 'human', 'presence', 'in', 'space', 'and', 'serves', 'as', 'a', 'laboratory', 'for', 'scientific', 'research', ',', 'demonstration', ',', 'and', 'technology', 'development', '.', 'It', 'consists', 'of', 'components', 'that', 'together', 'form', 'an', 'environment', 'where', 'no', 'terrestrial', 'forces', 'are', 'needed', 'for', 'crew', 'members', 'to', 'carry', 'out', 'their', 'tasks', '.', 'With', 'over', '13', 'trillion', 'cubic', 'metres', 'of', 'air', 'being', 'supplied', 'for', 'breathing', 'and', 'insulation', ',', 'it', 'surpasses', 'Earth', \"'s\", 'surface', 'and', 'is', 'by', 'far', 'the', 'largest', 'human-made', 'structure', 'in', 'space', '.', 'Due', 'to', 'the', 'magnitude', 'of', 'work', 'conducted', 'by', 'the', 'ISS', ',', 'disagreements', 'arose', 'between', 'nations', 'regarding', 'distribution', 'of', 'financial', 'contributions', '.', 'There', 'have', 'also', 'been', 'debates', 'about', 'which', 'country', 'she', 'represents', 'the', 'greatest', 'achievement', 'for', '.', 'This', 'paper', 'seeks', 'to', 'outline', 'the', 'reasons', 'why', 'the', 'National', 'Academy', 'of', 'Sciences', 'concluded', 'that', 'Hall', '’', 's', 'statement', 'was', 'false', 'and', 'caused', 'substantial', 'harm', '.', '[', 'Asking', 'how', 'it', 'causes', 'harm', ']', 'How', 'did', 'his']]\n",
      "Tokenized References: [[['Developed', 'countries', 'will', 'communicate', 'every', 'two', 'years', 'on', 'projected', 'levels', 'of', 'public', 'climate', 'finance', 'to', 'be', 'provided', 'to', 'developing', 'countries', '.', 'Developing', 'countries', 'will', 'report', 'on', 'climate', 'finance', 'they', 'will', 'provide', 'voluntarily', '.', 'Developed', 'countries', 'are', 'not', 'obligated', 'to', 'provide', 'climate', 'finance', ',', 'but', 'they', 'are', 'welcome', 'to', 'contribute', 'financing', 'for', 'climate', 'activities', '.']]]\n",
      "PEFT MODEL BLEU Score: 9.484203087232283e-232\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.05521472392638037, recall=0.18, fmeasure=0.08450704225352111), mid=Score(precision=0.05521472392638037, recall=0.18, fmeasure=0.08450704225352111), high=Score(precision=0.05521472392638037, recall=0.18, fmeasure=0.08450704225352111)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408), mid=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408), high=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408)), 'rougeLsum': AggregateScore(low=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408), mid=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408), high=Score(precision=0.03680981595092025, recall=0.12, fmeasure=0.05633802816901408))}\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s not too relevant to my question. By the way, is that related to last year’s conference?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Yes, it is.\n",
      "Tokenized Predictions: [['Yes', ',', 'it', 'is', '.']]\n",
      "Tokenized References: [[['The', 'Paris', 'Agreement', 'is', 'a', 'big', 'international', 'agreement', 'to', 'tackle', 'climate', 'change', '.', '195', 'countries', 'signed', 'the', 'agreement', 'in', 'December', '2015', 'at', 'the', '21st', 'Conference', 'of', 'the', 'Parties', '(', 'COP21', ')', 'and', '186', 'have', 'become', 'party', 'to', 'it', '.', 'The', 'long-term', 'goal', 'is', 'to', 'keep', 'the', 'global', 'average', 'temperature', 'well', 'below', '2C', 'above', 'pre-industrial', 'levels', '.', 'In', 'June', '2017', ',', 'U.S.', 'President', 'Donald', 'Trump', 'announced', 'his', 'intention', 'to', 'withdraw', 'the', 'United', 'States', 'from', 'the', 'agreement', '.']]]\n",
      "ORIGINAL MODEL BLEU Score: 1.173003147035209e-237\n",
      "ORIGINAL MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), mid=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), high=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), mid=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), high=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333)), 'rougeLsum': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), mid=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333), high=Score(precision=0.6666666666666666, recall=0.027777777777777776, fmeasure=0.05333333333333333))}\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The Paris Agreement is a big international agreement to tackle climate change. Countries are welcome to join any time, and they must commit to reducing their greenhouse gas emissions. All countries except the United States have pledged to cut carbon emissions between now and 2030. They are invited to increase their pledges every two years. Nations will also report on their projected levels of emissions every two years. Developed nations are expected to ramp up their commitments every two years starting in 2025. Developing nations are encouraged to consider increasing theirs around 2030. By the end of the agreement, countries should be committing to slash their greenhouse gas emissions by at least 50 percent below 2017 levels. This target represents a high-watermark for efforts, but scientists say achieving even more aggressive targets would further accelerate efforts to rein in global warming. The agreement was\n",
      "Tokenized Predictions: [['The', 'Paris', 'Agreement', 'is', 'a', 'big', 'international', 'agreement', 'to', 'tackle', 'climate', 'change', '.', 'Countries', 'are', 'welcome', 'to', 'join', 'any', 'time', ',', 'and', 'they', 'must', 'commit', 'to', 'reducing', 'their', 'greenhouse', 'gas', 'emissions', '.', 'All', 'countries', 'except', 'the', 'United', 'States', 'have', 'pledged', 'to', 'cut', 'carbon', 'emissions', 'between', 'now', 'and', '2030', '.', 'They', 'are', 'invited', 'to', 'increase', 'their', 'pledges', 'every', 'two', 'years', '.', 'Nations', 'will', 'also', 'report', 'on', 'their', 'projected', 'levels', 'of', 'emissions', 'every', 'two', 'years', '.', 'Developed', 'nations', 'are', 'expected', 'to', 'ramp', 'up', 'their', 'commitments', 'every', 'two', 'years', 'starting', 'in', '2025', '.', 'Developing', 'nations', 'are', 'encouraged', 'to', 'consider', 'increasing', 'theirs', 'around', '2030', '.', 'By', 'the', 'end', 'of', 'the', 'agreement', ',', 'countries', 'should', 'be', 'committing', 'to', 'slash', 'their', 'greenhouse', 'gas', 'emissions', 'by', 'at', 'least', '50', 'percent', 'below', '2017', 'levels', '.', 'This', 'target', 'represents', 'a', 'high-watermark', 'for', 'efforts', ',', 'but', 'scientists', 'say', 'achieving', 'even', 'more', 'aggressive', 'targets', 'would', 'further', 'accelerate', 'efforts', 'to', 'rein', 'in', 'global', 'warming', '.', 'The', 'agreement', 'was']]\n",
      "Tokenized References: [[['The', 'Paris', 'Agreement', 'is', 'a', 'big', 'international', 'agreement', 'to', 'tackle', 'climate', 'change', '.', '195', 'countries', 'signed', 'the', 'agreement', 'in', 'December', '2015', 'at', 'the', '21st', 'Conference', 'of', 'the', 'Parties', '(', 'COP21', ')', 'and', '186', 'have', 'become', 'party', 'to', 'it', '.', 'The', 'long-term', 'goal', 'is', 'to', 'keep', 'the', 'global', 'average', 'temperature', 'well', 'below', '2C', 'above', 'pre-industrial', 'levels', '.', 'In', 'June', '2017', ',', 'U.S.', 'President', 'Donald', 'Trump', 'announced', 'his', 'intention', 'to', 'withdraw', 'the', 'United', 'States', 'from', 'the', 'agreement', '.']]]\n",
      "PEFT MODEL BLEU Score: 0.10955752810171694\n",
      "PEFT MODEL ROUGE Scores: {'rouge1': AggregateScore(low=Score(precision=0.23448275862068965, recall=0.4722222222222222, fmeasure=0.31336405529953915), mid=Score(precision=0.23448275862068965, recall=0.4722222222222222, fmeasure=0.31336405529953915), high=Score(precision=0.23448275862068965, recall=0.4722222222222222, fmeasure=0.31336405529953915)), 'rouge2': AggregateScore(low=Score(precision=0.1111111111111111, recall=0.22535211267605634, fmeasure=0.1488372093023256), mid=Score(precision=0.1111111111111111, recall=0.22535211267605634, fmeasure=0.1488372093023256), high=Score(precision=0.1111111111111111, recall=0.22535211267605634, fmeasure=0.1488372093023256)), 'rougeL': AggregateScore(low=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766), mid=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766), high=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766)), 'rougeLsum': AggregateScore(low=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766), mid=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766), high=Score(precision=0.16551724137931034, recall=0.3333333333333333, fmeasure=0.22119815668202766))}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from datasets import load_metric\n",
    "\n",
    "# Ensure NLTK packages are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(predictions, references):\n",
    "    # Tokenize the predictions and references\n",
    "    tokenized_predictions = [nltk.word_tokenize(pred) for pred in predictions]\n",
    "    tokenized_references = [[nltk.word_tokenize(ref)] for ref in references]\n",
    "\n",
    "    # Print tokenization output for verification\n",
    "    print(\"Tokenized Predictions:\", tokenized_predictions)\n",
    "    print(\"Tokenized References:\", tokenized_references)\n",
    "\n",
    "    # Use NLTK's BLEU implementation\n",
    "    bleu_score_value = corpus_bleu(tokenized_references, tokenized_predictions)\n",
    "    return bleu_score_value\n",
    "\n",
    "# Function to calculate ROUGE score\n",
    "def calculate_rouge(predictions, references):\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    rouge_scores = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    return rouge_scores\n",
    "\n",
    "# Function to generate answer and evaluate with BLEU and ROUGE scores\n",
    "def generate_answer(query, reference):\n",
    "    system_prompt = \"\"\"Answer the following question truthfully.\n",
    "If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
    "If the question is too complex, respond 'Kindly, consult the author.'.\"\"\"\n",
    "\n",
    "    user_prompt = f\"<HUMAN>: {query}\\n<ASSISTANT>:\"\n",
    "\n",
    "    final_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    device = \"cuda:0\"\n",
    "    dashline = \"-\" * 50\n",
    "\n",
    "    # Tokenize and generate the response with the original model\n",
    "    encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input_ids=encoding.input_ids,\n",
    "                             max_length=256,  # Adjust max length as per your model's maximum generation capacity\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             eos_token_id=tokenizer.eos_token_id,\n",
    "                             attention_mask=encoding.attention_mask,\n",
    "                             temperature=0.6,\n",
    "                             top_p=0.7,\n",
    "                             repetition_penalty=1.2,\n",
    "                             num_return_sequences=1)\n",
    "    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    original_formatted_output = extract_response(text_output, query)\n",
    "\n",
    "    # Print original model question and response\n",
    "    print(dashline)\n",
    "    print(f'QUESTION:\\n{query}')\n",
    "    print(f'ORIGINAL MODEL RESPONSE:\\n{original_formatted_output}')\n",
    "\n",
    "    # Calculate BLEU score for original model\n",
    "    original_bleu_score = calculate_bleu([original_formatted_output], [reference])\n",
    "    print(f'ORIGINAL MODEL BLEU Score: {original_bleu_score}')\n",
    "\n",
    "    # Calculate ROUGE scores for original model\n",
    "    original_rouge_scores = calculate_rouge([original_formatted_output], [reference])\n",
    "    print(f'ORIGINAL MODEL ROUGE Scores: {original_rouge_scores}')\n",
    "\n",
    "    print(dashline)\n",
    "\n",
    "    # Tokenize and generate the response with the PEFT model\n",
    "    peft_encoding = peft_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids,\n",
    "                                       max_length=256,  # Adjust max length as per your model's maximum generation capacity\n",
    "                                       pad_token_id=peft_tokenizer.eos_token_id,\n",
    "                                       eos_token_id=peft_tokenizer.eos_token_id,\n",
    "                                       attention_mask=peft_encoding.attention_mask,\n",
    "                                       temperature=0.6,\n",
    "                                       top_p=0.7,\n",
    "                                       repetition_penalty=1.2,\n",
    "                                       num_return_sequences=1)\n",
    "    peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "    peft_formatted_output = extract_response(peft_text_output, query)\n",
    "\n",
    "    # Print PEFT model response\n",
    "    print(f'PEFT MODEL RESPONSE:\\n{peft_formatted_output}')\n",
    "\n",
    "    # Calculate BLEU score for PEFT model\n",
    "    peft_bleu_score = calculate_bleu([peft_formatted_output], [reference])\n",
    "    print(f'PEFT MODEL BLEU Score: {peft_bleu_score}')\n",
    "\n",
    "    # Calculate ROUGE scores for PEFT model\n",
    "    peft_rouge_scores = calculate_rouge([peft_formatted_output], [reference])\n",
    "    print(f'PEFT MODEL ROUGE Scores: {peft_rouge_scores}')\n",
    "\n",
    "    print(dashline)\n",
    "\n",
    "def extract_response(text_output, query):\n",
    "    parts = text_output.split(\"<HUMAN>:\")[1:]  # Skip the first empty part\n",
    "    for part in parts:\n",
    "        if part.startswith(f\" {query}\"):\n",
    "            assistant_idx = part.find(\"<ASSISTANT>:\")\n",
    "            assistant_response = part[assistant_idx + len(\"<ASSISTANT>:\"):].strip()\n",
    "            return assistant_response\n",
    "    return \"[No response available]\"\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"/teamspace/studios/this_studio/formatted_dialogue.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Restrict to the first 20 dialogues\n",
    "data = data[:20]\n",
    "\n",
    "# Evaluate on each query and reference\n",
    "for item in data:\n",
    "    dialogue = item[\"dialogue\"]\n",
    "    human_part = dialogue.split(\"<ASSISTANT>:\")[0].strip()\n",
    "    assistant_part = dialogue.split(\"<ASSISTANT>:\")[1].strip()\n",
    "    query = human_part.split(\"<HUMAN>:\")[1].strip()\n",
    "    reference = assistant_part.strip()\n",
    "    generate_answer(query, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating Average BLEU & ROUGE Score over the entire Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU score for Original Model: 0.0004989965366655631\n",
      "Average BLEU score for PEFT Model: 0.07361871207919557\n",
      "Average ROUGE score for Original Model: 0.06148177544238058\n",
      "Average ROUGE score for PEFT Model: 0.19468502717373629\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
    "from datasets import load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure NLTK packages are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(predictions, references):\n",
    "    # Tokenize the predictions and references\n",
    "    tokenized_predictions = [nltk.word_tokenize(pred) for pred in predictions]\n",
    "    tokenized_references = [[nltk.word_tokenize(ref)] for ref in references]\n",
    "\n",
    "    # Use NLTK's BLEU implementation\n",
    "    bleu_score_value = corpus_bleu(tokenized_references, tokenized_predictions)\n",
    "    return bleu_score_value\n",
    "\n",
    "# Function to calculate ROUGE score\n",
    "def calculate_rouge(predictions, references):\n",
    "    rouge_metric = load_metric(\"rouge\")\n",
    "    rouge_scores = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    return rouge_scores\n",
    "\n",
    "# Function to generate answer and evaluate with BLEU and ROUGE scores\n",
    "def generate_answer(query, reference):\n",
    "    system_prompt = \"\"\"Answer the following question truthfully.\n",
    "If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
    "If the question is too complex, respond 'Kindly, consult the author.'.\"\"\"\n",
    "\n",
    "    user_prompt = f\"<HUMAN>: {query}\\n<ASSISTANT>:\"\n",
    "\n",
    "    final_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    device = \"cuda:0\"\n",
    "    dashline = \"-\" * 50\n",
    "\n",
    "    # Tokenize and generate the response with the original model\n",
    "    encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input_ids=encoding.input_ids,\n",
    "                             max_length=256,  # Adjust max length as per your model's maximum generation capacity\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             eos_token_id=tokenizer.eos_token_id,\n",
    "                             attention_mask=encoding.attention_mask,\n",
    "                             temperature=0.6,\n",
    "                             top_p=0.7,\n",
    "                             repetition_penalty=1.2,\n",
    "                             num_return_sequences=1)\n",
    "    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    original_formatted_output = extract_response(text_output, query)\n",
    "\n",
    "    # Calculate BLEU score for original model\n",
    "    original_bleu_score = calculate_bleu([original_formatted_output], [reference])\n",
    "\n",
    "    # Calculate ROUGE scores for original model\n",
    "    original_rouge_scores = calculate_rouge([original_formatted_output], [reference])\n",
    "\n",
    "    # Tokenize and generate the response with the PEFT model\n",
    "    peft_encoding = peft_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids,\n",
    "                                       max_length=256,  # Adjust max length as per your model's maximum generation capacity\n",
    "                                       pad_token_id=peft_tokenizer.eos_token_id,\n",
    "                                       eos_token_id=peft_tokenizer.eos_token_id,\n",
    "                                       attention_mask=peft_encoding.attention_mask,\n",
    "                                       temperature=0.6,\n",
    "                                       top_p=0.7,\n",
    "                                       repetition_penalty=1.2,\n",
    "                                       num_return_sequences=1)\n",
    "    peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "    peft_formatted_output = extract_response(peft_text_output, query)\n",
    "\n",
    "    # Calculate BLEU score for PEFT model\n",
    "    peft_bleu_score = calculate_bleu([peft_formatted_output], [reference])\n",
    "\n",
    "    # Calculate ROUGE scores for PEFT model\n",
    "    peft_rouge_scores = calculate_rouge([peft_formatted_output], [reference])\n",
    "\n",
    "    return original_bleu_score, peft_bleu_score, original_rouge_scores, peft_rouge_scores\n",
    "\n",
    "def extract_response(text_output, query):\n",
    "    parts = text_output.split(\"<HUMAN>:\")[1:]  # Skip the first empty part\n",
    "    for part in parts:\n",
    "        if part.startswith(f\" {query}\"):\n",
    "            assistant_idx = part.find(\"<ASSISTANT>:\")\n",
    "            assistant_response = part[assistant_idx + len(\"<ASSISTANT>:\"):].strip()\n",
    "            return assistant_response\n",
    "    return \"[No response available]\"\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"/teamspace/studios/this_studio/formatted_dialogue.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "# Initialize lists to store BLEU and ROUGE scores\n",
    "original_bleu_scores = []\n",
    "peft_bleu_scores = []\n",
    "original_rouge_scores_list = []\n",
    "peft_rouge_scores_list = []\n",
    "\n",
    "# Evaluate on each query and reference\n",
    "for item in data:\n",
    "    dialogue = item[\"dialogue\"]\n",
    "    human_part = dialogue.split(\"<ASSISTANT>:\")[0].strip()\n",
    "    assistant_part = dialogue.split(\"<ASSISTANT>:\")[1].strip()\n",
    "    query = human_part.split(\"<HUMAN>:\")[1].strip()\n",
    "    reference = assistant_part.strip()\n",
    "    \n",
    "    original_bleu, peft_bleu, original_rouge, peft_rouge = generate_answer(query, reference)\n",
    "    \n",
    "    # Append the scores to the lists\n",
    "    original_bleu_scores.append(original_bleu)\n",
    "    peft_bleu_scores.append(peft_bleu)\n",
    "    original_rouge_scores_list.append(original_rouge['rougeL'].mid.fmeasure)\n",
    "    peft_rouge_scores_list.append(peft_rouge['rougeL'].mid.fmeasure)\n",
    "\n",
    "# Calculate average scores\n",
    "avg_original_bleu = sum(original_bleu_scores) / len(original_bleu_scores) if original_bleu_scores else 0\n",
    "avg_peft_bleu = sum(peft_bleu_scores) / len(peft_bleu_scores) if peft_bleu_scores else 0\n",
    "avg_original_rouge = sum(original_rouge_scores_list) / len(original_rouge_scores_list) if original_rouge_scores_list else 0\n",
    "avg_peft_rouge = sum(peft_rouge_scores_list) / len(peft_rouge_scores_list) if peft_rouge_scores_list else 0\n",
    "\n",
    "# Print average scores\n",
    "print(f\"Average BLEU score for Original Model: {avg_original_bleu}\")\n",
    "print(f\"Average BLEU score for PEFT Model: {avg_peft_bleu}\")\n",
    "print(f\"Average ROUGE score for Original Model: {avg_original_rouge}\")\n",
    "print(f\"Average ROUGE score for PEFT Model: {avg_peft_rouge}\")\n",
    "\n",
    "# Plot the average scores\n",
    "labels = ['BLEU', 'ROUGE']\n",
    "original_scores = [avg_original_bleu, avg_original_rouge]\n",
    "peft_scores = [avg_peft_bleu, avg_peft_rouge]\n",
    "\n",
    "x = range(len(labels))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x, original_scores, width=0.4, label='Original Model', align='center')\n",
    "ax.bar(x, peft_scores, width=0.4, label='PEFT Model', align='edge')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Average BLEU and ROUGE Scores')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Calculating BERTScore & MoverScore for first 20 dialogues in our Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "I remember Glasgow hosting COP26 last year, but unfortunately I was out of the loop. What was it about?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It was a conference on climate change.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.5039013028144836, R=-0.07908180356025696, F1=0.19719505310058594\n",
      "ORIGINAL MODEL MoverScore: 0.2547206289503811\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The COP26 event is a global united Nations summit about climate change and how countries are planning to tackle it. Name a conference or gathering you’d like to attend and we can find a way to make it happen for you with our bespoke coaching program. Even if you attended COP26 last year, this is relevant; as Climate Change is constantly evolving, with new data emerging almost every day. They made strides towards carbon neutrality and there is still lots more to do. It is a long-term journey which will continue to challenge many leaders moving forward. Sometimes the pressure from Climate Change could impact their ability to carry out business duties. These days they may be suffering from some kind of mental health issue due to the burden they are under. We would investigate this and look to provide support going forward. A stressor like this can cause both short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.09218805283308029, R=0.17239251732826233, F1=0.13338503241539001\n",
      "PEFT MODEL MoverScore: 0.009526759961918662\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Interesting. What are the effects of these changes?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "The effects of these changes are as follows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.04127012938261032, R=-0.2038775235414505, F1=-0.0826437845826149\n",
      "ORIGINAL MODEL MoverScore: 0.12407331142220603\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The first one is about increasing your website’s visibility. You see that when you search in Google with an address already specified, the entire page and all its results will be shown in the first result block, instead of just the link title, description and URL. This makes the organic result look more complete and attractive. Do you want to make your site visible on Internet? Then increase your SEO! By doing so, you also force users who browse the web by means of search engines to pay attention to your address rather than those of others. To raise awareness for your brand, it is better if the search engine shows a lot of information about it in the search result list. In addition, Google now displays more details of websites in the search results. With expanded presentation, more data can be conveyed. For example, visual elements, such as photos, videos, maps and local information, can be presented in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.11426553130149841, R=-0.16954609751701355, F1=-0.1402112990617752\n",
      "PEFT MODEL MoverScore: 0.011144180825880071\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s rather vague. Can you be more specific?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "I’m sorry, I don’t know the answer to this question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=-0.07609247416257858, R=-0.2149183303117752, F1=-0.1446523219347\n",
      "ORIGINAL MODEL MoverScore: 0.08746127293679756\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Answer one of the most common stock market questions: how do I calculate my ROE? And expand upon it; for example, by describing the components of your diet and why you chose them. Show genuine interest in their hobbies and passions and support them in pursuing them. No teenager wants to feel like they’re locked in a cage. They need to feel free to explore the world and find out about new things. Encourage original thinking. Teach them something new every day. Inspire them to discover their unique strengths and talents. The greatest athlete, the smartest student, the highest achieving person in academics are all great accolades but won’t ever mean anything if their soul is torn apart due to bullying inflicted because they’re different from the rest. Support them in finding their uniqueness and build their self-confidence. A teen who believes he/she is amazing will always rise above the naysayers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.14407242834568024, R=-0.14993244409561157, F1=-0.14514073729515076\n",
      "PEFT MODEL MoverScore: 0.003944164503454565\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Woah. They’re not all bad, right?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "No, they’re not.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=-0.00328809698112309, R=-0.18977594375610352, F1=-0.09654918313026428\n",
      "ORIGINAL MODEL MoverScore: 1.0\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "A biased algorithm is one which systematically makes errors or omissions in handling a group of data; however, algorithms are not inherently flawed. Some algorithms, such as Newton’s method, are very robust and reliable. In addition, an algorithm does not determine whether a mistake is intentional or accidental. An algorithm that has gone wrong due to an error or flaw can still be re-executed and correct the error. The term “algorithm crash” suggests that an algorithm falls apart and loses its ability to perform inference. This kind of failure would imply that the algorithm was fundamentaly broken and cannot be reused. Modern machine learning systems based on deep neural networks are highly susceptible to data noise, model initialization problems, and other easy-to-fall-apart issues. However, these problems do not characterize algorithm failures, they represent only a single problem in many ways. Algorithm crashes are cheap Problems that could\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.12047877907752991, R=-0.09524205327033997, F1=-0.1060926765203476\n",
      "PEFT MODEL MoverScore: 0.006696587519534818\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "I remember Glasgow hosting COP26 last year, but unfortunately I was out of the loop. What was it about?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It was a conference on climate change.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.5039013028144836, R=-0.07908180356025696, F1=0.19719505310058594\n",
      "ORIGINAL MODEL MoverScore: 0.2547206289503811\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The COP26 event is a global united Nations summit about climate change and how countries are planning to tackle it. Name a conference or gathering you’d like to attend and we can find a way to make it happen for you with our bespoke coaching program. Even if you attended COP26 last year, this is relevant; as Climate Change is constantly evolving, with new data emerging almost every day. They made strides towards carbon neutrality and there is still lots more to do. It is a long-term journey which will continue to challenge many leaders moving forward. Sometimes the pressure from Climate Change could impact their ability to carry out business duties. These days they may be suffering from some kind of mental health issue due to the burden they are under. We would investigate this and look to provide support going forward. A stressor like this can cause both short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.09218805283308029, R=0.17239251732826233, F1=0.13338503241539001\n",
      "PEFT MODEL MoverScore: 0.009526759961918662\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Interesting. What are the effects of these changes?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "The effects of these changes are as follows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.04127012938261032, R=-0.2038775235414505, F1=-0.0826437845826149\n",
      "ORIGINAL MODEL MoverScore: 0.12407331142220603\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The first one is about increasing your website’s visibility. You see that when you search in Google with an address already specified, the entire page and all its results will be shown in the first result block, instead of just the link title, description and URL. This makes the organic result look more complete and attractive. Do you want to make your site visible on Internet? Then increase your SEO! By doing so, you also force users who browse the web by means of search engines to pay attention to your address rather than those of others. To raise awareness for your brand, it is better if the search engine shows a lot of information about it in the search result list. In addition, Google now displays more details of websites in the search results. With expanded presentation, more data can be conveyed. For example, visual elements, such as photos, videos, maps and local information, can be presented in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.11426553130149841, R=-0.16954609751701355, F1=-0.1402112990617752\n",
      "PEFT MODEL MoverScore: 0.011144180825880071\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s interesting. Tell me more.\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "I’m sorry, I don’t know the answer to this question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=-0.05405178666114807, R=-0.18943707644939423, F1=-0.12087568640708923\n",
      "ORIGINAL MODEL MoverScore: 0.08476297844086428\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "A man named Phil McGraw was travelling through airports, noticing how nervous and anxious people were; he noted that a typical experience includes loud noises, crowded spaces, and general chaos. In those moments of panic, people tend to act without thought or reason. They have lost the ability to think clearly and rationally. This book spurred seminars, radio shows, and TV programs devoted to self-help for everyday problems. The defining feature of self-help is its attempt to create positive change in an individual's life by providing information and techniques. This kind of writing has become so common and widespread that it has spawned entire industries: bookstore shelves are packed with titles on the topic, and websites offer daily doses of self-help via e-mail subscriptions. While some articles and books aim to address serious issues (such as addiction, depression, and abuse), most focus on less-serious matters but still encourage readers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.05074847862124443, R=-0.09036136418581009, F1=-0.06889695674180984\n",
      "PEFT MODEL MoverScore: 0.003836784148338527\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Okay, but how does it affect developing countries?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It doesn't.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.07760581374168396, R=-0.19673512876033783, F1=-0.061703044921159744\n",
      "ORIGINAL MODEL MoverScore: 1.0\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Developing countries are those that are economically underdeveloped and who have a low level of technological advancement. They are the least able to adapt to climate change. Climate change causes them: Economic losses on account of reduced yields on agricultural lands due to changing patterns of precipitation; Losses in fisheries due to changes in temperature, acidity, and other alterations in the quality of water; The migration of people living in developing nations in search of better climatic conditions; Environmental degradation in the form of loss of vegetation leading to soil erosion; All these impacts of climate change cause about 400,000 deaths per year on average on account of hunger and diseases spread by insects. In addition, climate change also leads to political instability which results in additional costs for establishing peace. It is therefore, big problem for developing nations. Also, as a lot more population lives in developing nations, they are generally less able to adapt to climate change.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.23850925266742706, R=0.38139551877975464, F1=0.31010517477989197\n",
      "PEFT MODEL MoverScore: 0.01589650245594911\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "How are developed countries helping with that?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Well, they're helping by providing financial aid to developing countries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.36952444911003113, R=0.014793814159929752, F1=0.18733759224414825\n",
      "ORIGINAL MODEL MoverScore: 0.09163850408865673\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Well, according to the Paris Agreement, every two years, developed countries are expected to communicate the projected levels of public climate finance they provide to developing countries to mitigate and adapt to climate change. Some developing countries also contribute to climate finance as well. Already, Brazil’s President Dilma Rousseff said the country is considering contributing, joining other emerging economies like China, which pledged to provide $3.1 billion over three years. The U.S. hasn’t committed to climate finance yet, despite promises to provide diplomatic support to developing nations facing climate migration due to rising seas, according to Nicholas Watts, director of the National Academy of Sciences’ Climate Project. “The United States has historically been one of the least generous countries in providing climate finance,” Watts says. “I think it will be a big negative for the administration if the United States continues to lag on climate finance.” In addition to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.4853793680667877, R=0.919308066368103, F1=0.6944990158081055\n",
      "PEFT MODEL MoverScore: 1.0\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Are they meeting them?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Yes, they are.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.0509922131896019, R=-0.1356794685125351, F1=-0.04243240877985954\n",
      "ORIGINAL MODEL MoverScore: 0.2706173680658519\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "At the end of their work, the enthusiasts have created a very comprehensive three-book series based on the characters and universes. The books serve as encyclopedias of the franchises, containing information about the worlds and the characters, as well as various facts and trivia. They have also given fans a way to continue the universes in their own imaginations through fiction. Fans who read these books are encouraged to imagine their own continuities in the realms of comics, novels, and games. In addition, these books act as marketing tools by creating interest for more merchandise; if a book does well, it is likely that a movie or comic spin-off will be made. These books represent a financial investment which generates revenue far exceeding the cost of creation. It is therefore in the interests of companies to create these libraries of material which can potentially generate endless amounts of income. [1] The books have met with success, becoming some\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.11474238336086273, R=-0.04757628217339516, F1=-0.07963307946920395\n",
      "PEFT MODEL MoverScore: 0.010787925796543738\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s not too relevant to my question. By the way, is that related to last year’s conference?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Yes, it is.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.08383745700120926, R=-0.24221275746822357, F1=-0.08290724456310272\n",
      "ORIGINAL MODEL MoverScore: 0.27266322804792675\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The Paris Agreement is a big international agreement to tackle climate change. Countries are welcome to join negotiations to deepen their commitments to slash greenhouse gas emissions, and at the same time, all nations are expected to align their economies with the goal of limiting projected temperature increases to 2C above pre-industrial levels. The biggest polluters aren’t signing on: U.S. President Donald Trump has vowed to exit the deal as soon as possible. And some key emerging markets like China are not even part of the agreement. Yet, the day Congress was signed into law in December 2015. Some experts say it could be the catalyst for real action from other countries to cut carbon emissions as well. Dr. Richard Moss, associate professor of environmental studies at Bard College, said Paris represents an important milestone toward meeting those goals. “The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.18171697854995728, R=0.4117096960544586, F1=0.29533734917640686\n",
      "PEFT MODEL MoverScore: 0.024332017490470752\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "So what happens at each COP?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Well, the first thing that happens is that the delegates gather in the plenary hall.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.10451550036668777, R=-0.19044165313243866, F1=-0.04571504890918732\n",
      "ORIGINAL MODEL MoverScore: 0.04489216573427273\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The United Nations Climate Change Conference are yearly conferences held in the framework of the UNFCCC. Each conference is known as a COP. The objective of these conferences is to assess the second round of Kyoto Protocol emissions reduction commitments made by developed countries and the first set of obligations agreed upon by emerging economies starting with developing countries who are exempt from KP obligations. These conferences also serve as the basis for the fifth assessment report published by the Intergovernmental Panel on Climate Change. There have been a total of twenty-two COPs since the treaty came into force in 1994. Because climate change is a global problem that requires collective action by all nations, international consensus is considered essential for any successful agreement regarding greenhouse gas emission reductions. Despite efforts by President Donald Trump to withdraw the United States from the Paris Agreement, there has been widespread support for the accord among both Republican and Democratic presidents dating back to President George W. Bush's administration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.17863087356090546, R=0.31292614340782166, F1=0.24613764882087708\n",
      "PEFT MODEL MoverScore: 0.019160735811754308\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "How are the two different?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "The first one is a question that is not answered by the author.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.1255253553390503, R=-0.08935395628213882, F1=0.017348729074001312\n",
      "ORIGINAL MODEL MoverScore: 0.06576044656918248\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The above answers are for synchronous and event-driven approaches to web APIs. Event-based programming is more efficient than traditional synchronous scripting, but modern browsers have made asynchronous script execution nearly obsolete. ClickOnce eliminates the need for a server with HTTP capabilities which means that users who cannot run JavaScript, or whose browser does not support it, can access your application. With push technology, data pushed from one endpoint to another flows when it is needed rather than in constant background polling like scripts. Mobile applications using push technologies perform better and consume less battery life compared to their equivalent scripted counterparts. By sending only the changed or new data each time instead of everything all the time, click-signals are significantly lighter on bandwidth consumption. Compared to traditional scripting, push technologies are lightweight and require minimal overhead to execute. Unlike scripts that pull data constantly, push technologies only retrieve the information they need from an endpoint as soon as it is required.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.17989835143089294, R=-0.06560949981212616, F1=-0.12160461395978928\n",
      "PEFT MODEL MoverScore: 0.007345371241092753\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "I remember Glasgow hosting COP26 last year, but unfortunately I was out of the loop. What was it about?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It was a conference on climate change.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.5039013028144836, R=-0.07908180356025696, F1=0.19719505310058594\n",
      "ORIGINAL MODEL MoverScore: 0.2547206289503811\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The COP26 event is a global united Nations summit about climate change and how countries are planning to tackle it. Name a conference or gathering you’d like to attend and we can find a way to make it happen for you with our bespoke coaching program. Even if you attended COP26 last year, this is relevant; as Climate Change is constantly evolving, with new data emerging almost every day. They made strides towards carbon neutrality and there is still lots more to do. It is a long-term journey which will continue to challenge many leaders moving forward. Sometimes the pressure from Climate Change could impact their ability to carry out business duties. These days they may be suffering from some kind of mental health issue due to the burden they are under. We would investigate this and look to provide support going forward. A stressor like this can cause both short\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.09218805283308029, R=0.17239251732826233, F1=0.13338503241539001\n",
      "PEFT MODEL MoverScore: 0.009526759961918662\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Interesting. What are the effects of these changes?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "The effects of these changes are as follows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.04127012938261032, R=-0.2038775235414505, F1=-0.0826437845826149\n",
      "ORIGINAL MODEL MoverScore: 0.12407331142220603\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The first one is about increasing your website’s visibility. You see that when you search in Google with an address already specified, the entire page and all its results will be shown in the first result block, instead of just the link title, description and URL. This makes the organic result look more complete and attractive. Do you want to make your site visible on Internet? Then increase your SEO! By doing so, you also force users who browse the web by means of search engines to pay attention to your address rather than those of others. To raise awareness for your brand, it is better if the search engine shows a lot of information about it in the search result list. In addition, Google now displays more details of websites in the search results. With expanded presentation, more data can be conveyed. For example, visual elements, such as photos, videos, maps and local information, can be presented in\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.11426553130149841, R=-0.16954609751701355, F1=-0.1402112990617752\n",
      "PEFT MODEL MoverScore: 0.011144180825880071\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s interesting. Tell me more.\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "I’m sorry, I don’t know the answer to this question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=-0.05405178666114807, R=-0.18943707644939423, F1=-0.12087568640708923\n",
      "ORIGINAL MODEL MoverScore: 0.08476297844086428\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "A man named Phil McGraw was travelling through airports, noticing how nervous and anxious people were; he noted that a typical experience includes loud noises, crowded spaces, and general chaos. In those moments of panic, people tend to act without thought or reason. They have lost the ability to think clearly and rationally. This book spurred seminars, radio shows, and TV programs devoted to self-help for everyday problems. The defining feature of self-help is its attempt to create positive change in an individual's life by providing information and techniques. This kind of writing has become so common and widespread that it has spawned entire industries: bookstore shelves are packed with titles on the topic, and websites offer daily doses of self-help via e-mail subscriptions. While some articles and books aim to address serious issues (such as addiction, depression, and abuse), most focus on less-serious matters but still encourage readers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.05074847862124443, R=-0.09036136418581009, F1=-0.06889695674180984\n",
      "PEFT MODEL MoverScore: 0.003836784148338527\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Okay, but how does it affect developing countries?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "It doesn't.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.07760581374168396, R=-0.19673512876033783, F1=-0.061703044921159744\n",
      "ORIGINAL MODEL MoverScore: 1.0\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Developing countries are those that are economically underdeveloped and who have a low level of technological advancement. They are the least able to adapt to climate change. Climate change causes them: Economic losses on account of reduced yields on agricultural lands due to changing patterns of precipitation; Losses in fisheries due to changes in temperature, acidity, and other alterations in the quality of water; The migration of people living in developing nations in search of better climatic conditions; Environmental degradation in the form of loss of vegetation leading to soil erosion; All these impacts of climate change cause about 400,000 deaths per year on average on account of hunger and diseases spread by insects. In addition, climate change also leads to political instability which results in additional costs for establishing peace. It is therefore, big problem for developing nations. Also, as a lot more population lives in developing nations, they are generally less able to adapt to climate change.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.23850925266742706, R=0.38139551877975464, F1=0.31010517477989197\n",
      "PEFT MODEL MoverScore: 0.01589650245594911\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "How are developed countries helping with that?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Well, they're helping by providing financial aid to developing countries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.36952444911003113, R=0.014793814159929752, F1=0.18733759224414825\n",
      "ORIGINAL MODEL MoverScore: 0.09163850408865673\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "Well, according to the Paris Agreement, every two years, developed countries are expected to communicate the projected levels of public climate finance they provide to developing countries to mitigate and adapt to climate change. Some developing countries also contribute to climate finance as well. Already, Brazil’s President Dilma Rousseff said the country is considering contributing, joining other emerging economies like China, which pledged to provide $3.1 billion over three years. The U.S. hasn’t committed to climate finance yet, despite promises to provide diplomatic support to developing nations facing climate migration due to rising seas, according to Nicholas Watts, director of the National Academy of Sciences’ Climate Project. “The United States has historically been one of the least generous countries in providing climate finance,” Watts says. “I think it will be a big negative for the administration if the United States continues to lag on climate finance.” In addition to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.4853793680667877, R=0.919308066368103, F1=0.6944990158081055\n",
      "PEFT MODEL MoverScore: 1.0\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "Are they meeting them?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Yes, they are.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.0509922131896019, R=-0.1356794685125351, F1=-0.04243240877985954\n",
      "ORIGINAL MODEL MoverScore: 0.2706173680658519\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "At the end of their work, the enthusiasts have created a very comprehensive three-book series based on the characters and universes. The books serve as encyclopedias of the franchises, containing information about the worlds and the characters, as well as various facts and trivia. They have also given fans a way to continue the universes in their own imaginations through fiction. Fans who read these books are encouraged to imagine their own continuities in the realms of comics, novels, and games. In addition, these books act as marketing tools by creating interest for more merchandise; if a book does well, it is likely that a movie or comic spin-off will be made. These books represent a financial investment which generates revenue far exceeding the cost of creation. It is therefore in the interests of companies to create these libraries of material which can potentially generate endless amounts of income. [1] The books have met with success, becoming some\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=-0.11474238336086273, R=-0.04757628217339516, F1=-0.07963307946920395\n",
      "PEFT MODEL MoverScore: 0.010787925796543738\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "QUESTION:\n",
      "That’s not too relevant to my question. By the way, is that related to last year’s conference?\n",
      "ORIGINAL MODEL RESPONSE:\n",
      "Yes, it is.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL BERTScore: P=0.08383745700120926, R=-0.24221275746822357, F1=-0.08290724456310272\n",
      "ORIGINAL MODEL MoverScore: 0.27266322804792675\n",
      "--------------------------------------------------\n",
      "PEFT MODEL RESPONSE:\n",
      "The Paris Agreement is a big international agreement to tackle climate change. Countries are welcome to join negotiations to deepen their commitments to slash greenhouse gas emissions, and at the same time, all nations are expected to align their economies with the goal of limiting projected temperature increases to 2C above pre-industrial levels. The biggest polluters aren’t signing on: U.S. President Donald Trump has vowed to exit the deal as soon as possible. And some key emerging markets like China are not even part of the agreement. Yet, the day Congress was signed into law in December 2015. Some experts say it could be the catalyst for real action from other countries to cut carbon emissions as well. Dr. Richard Moss, associate professor of environmental studies at Bard College, said Paris represents an important milestone toward meeting those goals. “The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT MODEL BERTScore: P=0.18171697854995728, R=0.4117096960544586, F1=0.29533734917640686\n",
      "PEFT MODEL MoverScore: 0.024332017490470752\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from bert_score import score as bert_score\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Ensure NLTK packages are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to calculate BERTScore\n",
    "def calculate_bertscore(predictions, references):\n",
    "    P, R, F1 = bert_score(predictions, references, lang=\"en\", rescale_with_baseline=True)\n",
    "    return {\"P\": P.mean().item(), \"R\": R.mean().item(), \"F1\": F1.mean().item()}\n",
    "\n",
    "# Function to preprocess and tokenize text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text and remove stopwords\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Function to calculate MoverScore between two sentences\n",
    "def calculate_moverscore(sentence1, sentence2, word2vec_model):\n",
    "    tokens1 = preprocess_text(sentence1)\n",
    "    tokens2 = preprocess_text(sentence2)\n",
    "\n",
    "    # Get embeddings for each token in both sentences\n",
    "    embeddings1 = np.array([word2vec_model[token] for token in tokens1 if token in word2vec_model])\n",
    "    embeddings2 = np.array([word2vec_model[token] for token in tokens2 if token in word2vec_model])\n",
    "\n",
    "    # Compute distance matrix between embeddings\n",
    "    distance_matrix = np.zeros((len(embeddings1), len(embeddings2)))\n",
    "    for i, emb1 in enumerate(embeddings1):\n",
    "        for j, emb2 in enumerate(embeddings2):\n",
    "            distance_matrix[i, j] = euclidean(emb1, emb2)\n",
    "\n",
    "    # Compute optimal transport plan using linear sum assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
    "\n",
    "    # Calculate MoverScore\n",
    "    total_distance = distance_matrix[row_ind, col_ind].sum()\n",
    "    moverscore = 1 / (1 + total_distance)  # Normalize to get MoverScore between 0 and 1\n",
    "\n",
    "    return moverscore\n",
    "\n",
    "# Function to generate answer and evaluate with BERTScore and MoverScore\n",
    "def generate_answer(query, reference):\n",
    "    system_prompt = \"\"\"Answer the following question truthfully.\n",
    "    If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
    "    If the question is too complex, respond 'Kindly, consult the author.'.\"\"\"\n",
    "\n",
    "    user_prompt = f\"<HUMAN>: {query}\\n<ASSISTANT>:\"\n",
    "\n",
    "    final_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dashline = \"-\" * 50\n",
    "\n",
    "    encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(input_ids=encoding.input_ids,\n",
    "                             max_length=256,\n",
    "                             pad_token_id=tokenizer.eos_token_id,\n",
    "                             eos_token_id=tokenizer.eos_token_id,\n",
    "                             attention_mask=encoding.attention_mask,\n",
    "                             temperature=0.6,\n",
    "                             top_p=0.7,\n",
    "                             repetition_penalty=1.2,\n",
    "                             num_return_sequences=1)\n",
    "    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    original_formatted_output = extract_response(text_output, query)\n",
    "\n",
    "    # Print original model question and response\n",
    "    print(dashline)\n",
    "    print(f'QUESTION:\\n{query}')\n",
    "    print(f'ORIGINAL MODEL RESPONSE:\\n{original_formatted_output}')\n",
    "\n",
    "    # Calculate BERTScore for original model\n",
    "    original_bertscore = calculate_bertscore([original_formatted_output], [reference])\n",
    "    print(f'ORIGINAL MODEL BERTScore: P={original_bertscore[\"P\"]}, R={original_bertscore[\"R\"]}, F1={original_bertscore[\"F1\"]}')\n",
    "\n",
    "    # Calculate MoverScore for original model\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "    original_moverscore = calculate_moverscore(original_formatted_output, reference, word2vec_model)\n",
    "    print(f'ORIGINAL MODEL MoverScore: {original_moverscore}')\n",
    "\n",
    "    print(dashline)\n",
    "\n",
    "    peft_encoding = peft_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "    peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids,\n",
    "                                        max_length=256,\n",
    "                                        pad_token_id=peft_tokenizer.eos_token_id,\n",
    "                                        eos_token_id=peft_tokenizer.eos_token_id,\n",
    "                                        attention_mask=peft_encoding.attention_mask,\n",
    "                                        temperature=0.6,\n",
    "                                        top_p=0.7,\n",
    "                                        repetition_penalty=1.2,\n",
    "                                        num_return_sequences=1)\n",
    "    peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "    peft_formatted_output = extract_response(peft_text_output, query)\n",
    "\n",
    "    # Print PEFT model response\n",
    "    print(f'PEFT MODEL RESPONSE:\\n{peft_formatted_output}')\n",
    "\n",
    "    # Calculate BERTScore for PEFT model\n",
    "    peft_bertscore = calculate_bertscore([peft_formatted_output], [reference])\n",
    "    print(f'PEFT MODEL BERTScore: P={peft_bertscore[\"P\"]}, R={peft_bertscore[\"R\"]}, F1={peft_bertscore[\"F1\"]}')\n",
    "\n",
    "    # Calculate MoverScore for PEFT model\n",
    "    peft_moverscore = calculate_moverscore(peft_formatted_output, reference, word2vec_model)\n",
    "    print(f'PEFT MODEL MoverScore: {peft_moverscore}')\n",
    "\n",
    "    print(dashline)\n",
    "\n",
    "\n",
    "def extract_response(text_output, query):\n",
    "    parts = text_output.split(\"<HUMAN>:\")[1:]  # Skip the first empty part\n",
    "    for part in parts:\n",
    "        if part.startswith(f\" {query}\"):\n",
    "            assistant_idx = part.find(\"<ASSISTANT>:\")\n",
    "            assistant_response = part[assistant_idx + len(\"<ASSISTANT>:\"):].strip()\n",
    "            return assistant_response\n",
    "    return \"[No response available]\"\n",
    "\n",
    "\n",
    "# Load your dataset\n",
    "with open(\"/teamspace/studios/this_studio/formatted_dialogue.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Restrict to the first 20 dialogues\n",
    "data = data[:20]\n",
    "\n",
    "# Evaluate on each query and reference\n",
    "for item in data:\n",
    "    dialogue = item[\"dialogue\"]\n",
    "    human_part = dialogue.split(\"<ASSISTANT>:\")[0].strip()\n",
    "    assistant_part = dialogue.split(\"<ASSISTANT>:\")[1].strip()\n",
    "    query = human_part.split(\"<HUMAN>:\")[1].strip()\n",
    "    reference = assistant_part.strip()\n",
    "    generate_answer(query, reference)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
