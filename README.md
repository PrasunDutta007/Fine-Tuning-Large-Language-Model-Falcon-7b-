# Fine Tuning Large Language Model Falcon-7B

<img src="Screenshots/falcon.jpg" width="100%">

## Requirements 

##### 1) Decent GPU with minimum 6GB VRAM
##### 2) Jupyter Notebook
##### 3) TREC CASsT Dataset

## Concepts Used

- PEFT: Parameter Efficient Fine Tuning
- LoRA: Low Rank Adaptation
- QLoRA: Quantization & Low Rank Adapters
- Evaluation Metrics
  - BLEU: Bilingual Evaluation Understudy
  - ROUGE: Recall-Oriented Understudy for Gisting Evaluation
  - BERTScore
  - MoverScore

 &rarr; Refer to the 3 [PPTs](PPTs/) for a detailed explaination related to concepts, fine-tuning procedure and evaluation metrics.

 


